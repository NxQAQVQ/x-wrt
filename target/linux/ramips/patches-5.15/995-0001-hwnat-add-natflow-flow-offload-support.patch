From fe2fbdf9cebd942bd9d1f7d8243107ffcb9a81c2 Mon Sep 17 00:00:00 2001
From: Chen Minqiang <ptpt52@gmail.com>
Date: Fri, 3 Feb 2023 17:53:31 +0800
Subject: [PATCH] hwnat: add natflow flow offload support

---
 drivers/net/ethernet/mediatek/Makefile        |   2 +-
 drivers/net/ethernet/mediatek/mtk_eth_soc.c   | 270 +++++--
 drivers/net/ethernet/mediatek/mtk_eth_soc.h   |  31 +-
 drivers/net/ethernet/mediatek/mtk_ppe.h       |  38 +-
 drivers/net/ethernet/mediatek/mtk_ppe1.c      | 762 ++++++++++++++++++
 .../net/ethernet/mediatek/mtk_ppe_debugfs.c   |   9 +-
 .../net/ethernet/mediatek/mtk_ppe_offload1.c  | 450 +++++++++++
 drivers/net/ethernet/mediatek/mtk_ppe_regs.h  |   5 +
 drivers/net/ethernet/mediatek/mtk_wed.c       |   2 +-
 drivers/net/ppp/ppp_generic.c                 |  29 +
 drivers/net/ppp/pppoe.c                       |  29 +
 include/linux/netdevice.h                     |  19 +
 include/linux/ppp_channel.h                   |   3 +
 include/net/netfilter/nf_flow_table.h         |  53 ++
 net/8021q/vlan_dev.c                          |  25 +
 net/bridge/br_device.c                        |  28 +
 net/dsa/slave.c                               |  27 +
 17 files changed, 1686 insertions(+), 96 deletions(-)
 create mode 100644 drivers/net/ethernet/mediatek/mtk_ppe1.c
 create mode 100644 drivers/net/ethernet/mediatek/mtk_ppe_offload1.c

--- a/drivers/net/ethernet/mediatek/Makefile
+++ b/drivers/net/ethernet/mediatek/Makefile
@@ -4,7 +4,7 @@
 #
 
 obj-$(CONFIG_NET_MEDIATEK_SOC) += mtk_eth.o
-mtk_eth-y := mtk_eth_soc.o mtk_sgmii.o mtk_eth_path.o mtk_ppe.o mtk_ppe_debugfs.o mtk_ppe_offload.o
+mtk_eth-y := mtk_eth_soc.o mtk_sgmii.o mtk_eth_path.o mtk_ppe1.o mtk_ppe_debugfs.o mtk_ppe_offload1.o
 mtk_eth-$(CONFIG_NET_MEDIATEK_SOC_WED) += mtk_wed.o mtk_wed_mcu.o mtk_wed_wo.o
 ifdef CONFIG_DEBUG_FS
 mtk_eth-$(CONFIG_NET_MEDIATEK_SOC_WED) += mtk_wed_debugfs.o
--- a/drivers/net/ethernet/mediatek/mtk_eth_soc.c
+++ b/drivers/net/ethernet/mediatek/mtk_eth_soc.c
@@ -24,6 +24,8 @@
 #include <linux/bitfield.h>
 #include <net/dsa.h>
 #include <net/dst_metadata.h>
+#include <linux/netfilter.h>
+#include <net/netfilter/nf_flow_table.h>
 
 #include "mtk_eth_soc.h"
 #include "mtk_wed.h"
@@ -581,6 +583,7 @@ static void mtk_mac_link_down(struct phy
 	mtk_w32(mac->hw, mcr, MTK_MAC_MCR(mac->id));
 }
 
+#if 0
 static void mtk_set_queue_speed(struct mtk_eth *eth, unsigned int idx,
 				int speed)
 {
@@ -649,6 +652,7 @@ static void mtk_set_queue_speed(struct m
 	ofs = MTK_QTX_OFFSET * idx;
 	mtk_w32(eth, val, soc->reg_map->qdma.qtx_sch + ofs);
 }
+#endif
 
 static void mtk_mac_link_up(struct phylink_config *config,
 			    struct phy_device *phy,
@@ -675,7 +679,7 @@ static void mtk_mac_link_up(struct phyli
 		break;
 	}
 
-	mtk_set_queue_speed(mac->hw, mac->id, speed);
+	//mtk_set_queue_speed(mac->hw, mac->id, speed);
 
 	/* Configure duplex */
 	if (duplex == DUPLEX_FULL)
@@ -976,7 +980,7 @@ static int mtk_init_fq_dma(struct mtk_et
 {
 	const struct mtk_soc_data *soc = eth->soc;
 	dma_addr_t phy_ring_tail;
-	int cnt = MTK_QDMA_RING_SIZE;
+	int cnt = MTK_DMA_SIZE;
 	dma_addr_t dma_addr;
 	int i;
 
@@ -1135,8 +1139,7 @@ static void mtk_tx_set_dma_desc_v1(struc
 
 	WRITE_ONCE(desc->txd1, info->addr);
 
-	data = TX_DMA_SWC | TX_DMA_PLEN0(info->size) |
-	       FIELD_PREP(TX_DMA_PQID, info->qid);
+	data = TX_DMA_SWC | TX_DMA_PLEN0(info->size);
 	if (info->last)
 		data |= TX_DMA_LS0;
 	WRITE_ONCE(desc->txd3, data);
@@ -1151,7 +1154,17 @@ static void mtk_tx_set_dma_desc_v1(struc
 		/* vlan header offload */
 		if (info->vlan)
 			data |= TX_DMA_INS_VLAN | info->vlan_tci;
+		if (info->natflow) {
+			if (mac->id && !info->vlan) {
+				data |= TX_DMA_INS_VLAN | 1;
+			}
+		}
 	}
+	if (info->natflow) {
+		data &= ~(TX_DMA_FPORT_MASK << TX_DMA_FPORT_SHIFT);
+		data |= (0x4 & TX_DMA_FPORT_MASK) << TX_DMA_FPORT_SHIFT;
+	}
+
 	WRITE_ONCE(desc->txd4, data);
 }
 
@@ -1170,8 +1183,15 @@ static void mtk_tx_set_dma_desc_v2(struc
 		data |= TX_DMA_LS0;
 	WRITE_ONCE(desc->txd3, data);
 
+	if (!info->qid && mac->id)
+		info->qid = MTK_QDMA_GMAC2_QID;
+
 	data = (mac->id + 1) << TX_DMA_FPORT_SHIFT_V2; /* forward port */
 	data |= TX_DMA_SWC_V2 | QID_BITS_V2(info->qid);
+	if (info->natflow) {
+		data &= ~(TX_DMA_FPORT_MASK_V2 << TX_DMA_FPORT_SHIFT_V2);
+		data |= (0x3 & TX_DMA_FPORT_MASK_V2) << TX_DMA_FPORT_SHIFT_V2;
+	}
 	WRITE_ONCE(desc->txd4, data);
 
 	data = 0;
@@ -1187,6 +1207,11 @@ static void mtk_tx_set_dma_desc_v2(struc
 	data = 0;
 	if (info->first && info->vlan)
 		data |= TX_DMA_INS_VLAN_V2 | info->vlan_tci;
+	if (info->first && info->natflow) {
+		if (mac->id && !info->vlan) {
+			data |= TX_DMA_INS_VLAN_V2 | 1;
+		}
+	}
 	WRITE_ONCE(desc->txd6, data);
 
 	WRITE_ONCE(desc->txd7, 0);
@@ -1213,12 +1238,12 @@ static int mtk_tx_map(struct sk_buff *sk
 		.gso = gso,
 		.csum = skb->ip_summed == CHECKSUM_PARTIAL,
 		.vlan = skb_vlan_tag_present(skb),
-		.qid = skb_get_queue_mapping(skb),
+		.qid = skb->mark & MTK_QDMA_TX_MASK,
 		.vlan_tci = skb_vlan_tag_get(skb),
 		.first = true,
 		.last = !skb_is_nonlinear(skb),
+		.natflow = ((skb->mark & HWNAT_QUEUE_MAPPING_MAGIC_MASK) == HWNAT_QUEUE_MAPPING_MAGIC && (skb->hash & HWNAT_QUEUE_MAPPING_MAGIC_MASK) == HWNAT_QUEUE_MAPPING_MAGIC),
 	};
-	struct netdev_queue *txq;
 	struct mtk_mac *mac = netdev_priv(dev);
 	struct mtk_eth *eth = mac->hw;
 	const struct mtk_soc_data *soc = eth->soc;
@@ -1226,10 +1251,8 @@ static int mtk_tx_map(struct sk_buff *sk
 	struct mtk_tx_dma *itxd_pdma, *txd_pdma;
 	struct mtk_tx_buf *itx_buf, *tx_buf;
 	int i, n_desc = 1;
-	int queue = skb_get_queue_mapping(skb);
 	int k = 0;
 
-	txq = netdev_get_tx_queue(dev, queue);
 	itxd = ring->next_free;
 	itxd_pdma = qdma_to_pdma(ring, itxd);
 	if (itxd == ring->last_free)
@@ -1278,7 +1301,7 @@ static int mtk_tx_map(struct sk_buff *sk
 			memset(&txd_info, 0, sizeof(struct mtk_tx_dma_desc_info));
 			txd_info.size = min_t(unsigned int, frag_size,
 					      soc->txrx.dma_max_len);
-			txd_info.qid = queue;
+			txd_info.qid = skb->mark & MTK_QDMA_TX_MASK;
 			txd_info.last = i == skb_shinfo(skb)->nr_frags - 1 &&
 					!(frag_size - txd_info.size);
 			txd_info.addr = skb_frag_dma_map(eth->dma_dev, frag,
@@ -1317,7 +1340,7 @@ static int mtk_tx_map(struct sk_buff *sk
 			txd_pdma->txd2 |= TX_DMA_LS1;
 	}
 
-	netdev_tx_sent_queue(txq, skb->len);
+	netdev_sent_queue(dev, skb->len);
 	skb_tx_timestamp(skb);
 
 	ring->next_free = mtk_qdma_phys_to_virt(ring, txd->txd2);
@@ -1329,7 +1352,8 @@ static int mtk_tx_map(struct sk_buff *sk
 	wmb();
 
 	if (MTK_HAS_CAPS(soc->caps, MTK_QDMA)) {
-		if (netif_xmit_stopped(txq) || !netdev_xmit_more())
+		if (netif_xmit_stopped(netdev_get_tx_queue(dev, 0)) ||
+		    !netdev_xmit_more())
 			mtk_w32(eth, txd->txd2, soc->reg_map->qdma.ctx_ptr);
 	} else {
 		int next_idx;
@@ -1398,7 +1422,7 @@ static void mtk_wake_queue(struct mtk_et
 	for (i = 0; i < MTK_MAC_COUNT; i++) {
 		if (!eth->netdev[i])
 			continue;
-		netif_tx_wake_all_queues(eth->netdev[i]);
+		netif_wake_queue(eth->netdev[i]);
 	}
 }
 
@@ -1438,7 +1462,7 @@ static netdev_tx_t mtk_start_xmit(struct
 
 	tx_num = mtk_cal_txd_req(eth, skb);
 	if (unlikely(atomic_read(&ring->free_count) <= tx_num)) {
-		netif_tx_stop_all_queues(dev);
+		netif_stop_queue(dev);
 		netif_err(eth, tx_queued, dev,
 			  "Tx Ring full when queue awake!\n");
 		spin_unlock(&eth->page_lock);
@@ -1482,7 +1506,7 @@ static netdev_tx_t mtk_start_xmit(struct
 	}
 
 	if (unlikely(atomic_read(&ring->free_count) <= ring->thresh))
-		netif_tx_stop_all_queues(dev);
+		netif_stop_queue(dev);
 
 	spin_unlock(&eth->page_lock);
 
@@ -1865,7 +1889,6 @@ static int mtk_poll_rx(struct napi_struc
 		unsigned int pktlen, *rxdcsum;
 		struct net_device *netdev;
 		dma_addr_t dma_addr;
-		u32 hash, reason;
 		int mac = 0;
 
 		ring = mtk_get_rx_ring(eth);
@@ -1977,18 +2000,8 @@ static int mtk_poll_rx(struct napi_struc
 		bytes += skb->len;
 
 		if (MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2)) {
-			reason = FIELD_GET(MTK_RXD5_PPE_CPU_REASON, trxd.rxd5);
-			hash = trxd.rxd5 & MTK_RXD5_FOE_ENTRY;
-			if (hash != MTK_RXD5_FOE_ENTRY)
-				skb_set_hash(skb, jhash_1word(hash, 0),
-					     PKT_HASH_TYPE_L4);
 			rxdcsum = &trxd.rxd3;
 		} else {
-			reason = FIELD_GET(MTK_RXD4_PPE_CPU_REASON, trxd.rxd4);
-			hash = trxd.rxd4 & MTK_RXD4_FOE_ENTRY;
-			if (hash != MTK_RXD4_FOE_ENTRY)
-				skb_set_hash(skb, jhash_1word(hash, 0),
-					     PKT_HASH_TYPE_L4);
 			rxdcsum = &trxd.rxd4;
 		}
 
@@ -1998,6 +2011,7 @@ static int mtk_poll_rx(struct napi_struc
 			skb_checksum_none_assert(skb);
 		skb->protocol = eth_type_trans(skb, netdev);
 
+#if 0
 		/* When using VLAN untagging in combination with DSA, the
 		 * hardware treats the MTK special tag as a VLAN and untags it.
 		 */
@@ -2015,6 +2029,42 @@ static int mtk_poll_rx(struct napi_struc
 
 		skb_record_rx_queue(skb, 0);
 		napi_gro_receive(napi, skb);
+#else
+		if (netdev->features & NETIF_F_HW_VLAN_CTAG_RX) {
+			if (MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2)) {
+				if (trxd.rxd3 & RX_DMA_VTAG_V2)
+					__vlan_hwaccel_put_tag(skb,
+						htons(RX_DMA_VPID(trxd.rxd4)),
+						RX_DMA_VID(trxd.rxd4));
+			} else if (trxd.rxd2 & RX_DMA_VTAG) {
+				__vlan_hwaccel_put_tag(skb, htons(RX_DMA_VPID(trxd.rxd3)),
+						       RX_DMA_VID(trxd.rxd3));
+			}
+
+			/* If the device is attached to a dsa switch, the special
+			 * tag inserted in VLAN field by hw switch can * be offloaded
+			 * by RX HW VLAN offload. Clear vlan info.
+			 */
+			if (netdev_uses_dsa(netdev))
+				__vlan_hwaccel_clear_tag(skb);
+		}
+
+		if (MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2)) {
+			if (mtk_offload_check_rx_v2(eth, skb, trxd.rxd5) == 0) {
+				skb_record_rx_queue(skb, 0);
+				napi_gro_receive(napi, skb);
+			} else {
+				dev_kfree_skb(skb);
+			}
+		} else {
+			if (mtk_offload_check_rx(eth, skb, trxd.rxd4) == 0) {
+				skb_record_rx_queue(skb, 0);
+				napi_gro_receive(napi, skb);
+			} else {
+				dev_kfree_skb(skb);
+			}
+		}
+#endif
 
 skip_rx:
 		ring->data[idx] = new_data;
@@ -2050,6 +2100,7 @@ rx_done:
 	return done;
 }
 
+#if 0
 struct mtk_poll_state {
     struct netdev_queue *txq;
     unsigned int total;
@@ -2087,9 +2138,10 @@ mtk_poll_tx_done(struct mtk_eth *eth, st
 	state->done = 1;
 	state->bytes = bytes;
 }
+#endif
 
 static int mtk_poll_tx_qdma(struct mtk_eth *eth, int budget,
-			    struct mtk_poll_state *state)
+			    unsigned int *done, unsigned int *bytes)
 {
 	const struct mtk_reg_map *reg_map = eth->soc->reg_map;
 	struct mtk_tx_ring *ring = &eth->tx_ring;
@@ -2119,9 +2171,12 @@ static int mtk_poll_tx_qdma(struct mtk_e
 			break;
 
 		if (tx_buf->data != (void *)MTK_DMA_DUMMY_DESC) {
-			if (tx_buf->type == MTK_TYPE_SKB)
-				mtk_poll_tx_done(eth, state, mac, tx_buf->data);
+			if (tx_buf->type == MTK_TYPE_SKB) {
+				struct sk_buff *skb = tx_buf->data;
 
+				bytes[mac] += skb->len;
+				done[mac]++;
+			}
 			budget--;
 		}
 		mtk_tx_unmap(eth, tx_buf, true);
@@ -2139,7 +2194,7 @@ static int mtk_poll_tx_qdma(struct mtk_e
 }
 
 static int mtk_poll_tx_pdma(struct mtk_eth *eth, int budget,
-			    struct mtk_poll_state *state)
+			    unsigned int *done, unsigned int *bytes)
 {
 	struct mtk_tx_ring *ring = &eth->tx_ring;
 	struct mtk_tx_buf *tx_buf;
@@ -2155,8 +2210,12 @@ static int mtk_poll_tx_pdma(struct mtk_e
 			break;
 
 		if (tx_buf->data != (void *)MTK_DMA_DUMMY_DESC) {
-			if (tx_buf->type == MTK_TYPE_SKB)
-				mtk_poll_tx_done(eth, state, 0, tx_buf->data);
+			if (tx_buf->type == MTK_TYPE_SKB) {
+				struct sk_buff *skb = tx_buf->data;
+
+				bytes[0] += skb->len;
+				done[0]++;
+			}
 			budget--;
 		}
 		mtk_tx_unmap(eth, tx_buf, true);
@@ -2177,15 +2236,26 @@ static int mtk_poll_tx(struct mtk_eth *e
 {
 	struct mtk_tx_ring *ring = &eth->tx_ring;
 	struct dim_sample dim_sample = {};
-	struct mtk_poll_state state = {};
+	unsigned int done[MTK_MAX_DEVS];
+	unsigned int bytes[MTK_MAX_DEVS];
+	int total = 0, i;
+
+	memset(done, 0, sizeof(done));
+	memset(bytes, 0, sizeof(bytes));
 
 	if (MTK_HAS_CAPS(eth->soc->caps, MTK_QDMA))
-		budget = mtk_poll_tx_qdma(eth, budget, &state);
+		budget = mtk_poll_tx_qdma(eth, budget, done, bytes);
 	else
-		budget = mtk_poll_tx_pdma(eth, budget, &state);
+		budget = mtk_poll_tx_pdma(eth, budget, done, bytes);
 
-	if (state.txq)
-		netdev_tx_completed_queue(state.txq, state.done, state.bytes);
+	for (i = 0; i < MTK_MAC_COUNT; i++) {
+		if (!eth->netdev[i] || !done[i])
+			continue;
+		netdev_completed_queue(eth->netdev[i], done[i], bytes[i]);
+		total += done[i];
+		eth->tx_packets += done[i];
+		eth->tx_bytes += bytes[i];
+	}
 
 	dim_update_sample(eth->tx_events, eth->tx_packets, eth->tx_bytes,
 			  &dim_sample);
@@ -2195,7 +2265,7 @@ static int mtk_poll_tx(struct mtk_eth *e
 	    (atomic_read(&ring->free_count) > ring->thresh))
 		mtk_wake_queue(eth);
 
-	return state.total;
+	return total;
 }
 
 static void mtk_handle_status_irq(struct mtk_eth *eth)
@@ -2280,26 +2350,19 @@ static int mtk_tx_alloc(struct mtk_eth *
 	struct mtk_tx_ring *ring = &eth->tx_ring;
 	int i, sz = soc->txrx.txd_size;
 	struct mtk_tx_dma_v2 *txd;
-	int ring_size;
-	u32 ofs, val;
 
-	if (MTK_HAS_CAPS(soc->caps, MTK_QDMA))
-		ring_size = MTK_QDMA_RING_SIZE;
-	else
-		ring_size = MTK_DMA_SIZE;
-
-	ring->buf = kcalloc(ring_size, sizeof(*ring->buf),
+	ring->buf = kcalloc(MTK_DMA_SIZE, sizeof(*ring->buf),
 			       GFP_KERNEL);
 	if (!ring->buf)
 		goto no_tx_mem;
 
-	ring->dma = dma_alloc_coherent(eth->dma_dev, ring_size * sz,
+	ring->dma = dma_alloc_coherent(eth->dma_dev, MTK_DMA_SIZE * sz,
 				       &ring->phys, GFP_KERNEL);
 	if (!ring->dma)
 		goto no_tx_mem;
 
-	for (i = 0; i < ring_size; i++) {
-		int next = (i + 1) % ring_size;
+	for (i = 0; i < MTK_DMA_SIZE; i++) {
+		int next = (i + 1) % MTK_DMA_SIZE;
 		u32 next_ptr = ring->phys + next * sz;
 
 		txd = ring->dma + i * sz;
@@ -2319,22 +2382,22 @@ static int mtk_tx_alloc(struct mtk_eth *
 	 * descriptors in ring->dma_pdma.
 	 */
 	if (!MTK_HAS_CAPS(soc->caps, MTK_QDMA)) {
-		ring->dma_pdma = dma_alloc_coherent(eth->dma_dev, ring_size * sz,
+		ring->dma_pdma = dma_alloc_coherent(eth->dma_dev, MTK_DMA_SIZE * sz,
 						    &ring->phys_pdma, GFP_KERNEL);
 		if (!ring->dma_pdma)
 			goto no_tx_mem;
 
-		for (i = 0; i < ring_size; i++) {
+		for (i = 0; i < MTK_DMA_SIZE; i++) {
 			ring->dma_pdma[i].txd2 = TX_DMA_DESP2_DEF;
 			ring->dma_pdma[i].txd4 = 0;
 		}
 	}
 
-	ring->dma_size = ring_size;
-	atomic_set(&ring->free_count, ring_size - 2);
+	ring->dma_size = MTK_DMA_SIZE;
+	atomic_set(&ring->free_count, MTK_DMA_SIZE - 2);
 	ring->next_free = ring->dma;
 	ring->last_free = (void *)txd;
-	ring->last_free_ptr = (u32)(ring->phys + ((ring_size - 1) * sz));
+	ring->last_free_ptr = (u32)(ring->phys + ((MTK_DMA_SIZE - 1) * sz));
 	ring->thresh = MAX_SKB_FRAGS;
 
 	/* make sure that all changes to the dma ring are flushed before we
@@ -2346,10 +2409,12 @@ static int mtk_tx_alloc(struct mtk_eth *
 		mtk_w32(eth, ring->phys, soc->reg_map->qdma.ctx_ptr);
 		mtk_w32(eth, ring->phys, soc->reg_map->qdma.dtx_ptr);
 		mtk_w32(eth,
-			ring->phys + ((ring_size - 1) * sz),
+			ring->phys + ((MTK_DMA_SIZE - 1) * sz),
 			soc->reg_map->qdma.crx_ptr);
 		mtk_w32(eth, ring->last_free_ptr, soc->reg_map->qdma.drx_ptr);
-
+		mtk_w32(eth, (QDMA_RES_THRES << 8) | QDMA_RES_THRES,
+			soc->reg_map->qdma.qtx_cfg);
+#if 0
 		for (i = 0, ofs = 0; i < MTK_QDMA_NUM_QUEUES; i++) {
 			val = (QDMA_RES_THRES << 8) | QDMA_RES_THRES;
 			mtk_w32(eth, val, soc->reg_map->qdma.qtx_cfg + ofs);
@@ -2368,9 +2433,10 @@ static int mtk_tx_alloc(struct mtk_eth *
 		mtk_w32(eth, val, soc->reg_map->qdma.tx_sch_rate);
 		if (MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2))
 			mtk_w32(eth, val, soc->reg_map->qdma.tx_sch_rate + 4);
+#endif
 	} else {
 		mtk_w32(eth, ring->phys_pdma, MT7628_TX_BASE_PTR0);
-		mtk_w32(eth, ring_size, MT7628_TX_MAX_CNT0);
+		mtk_w32(eth, MTK_DMA_SIZE, MT7628_TX_MAX_CNT0);
 		mtk_w32(eth, 0, MT7628_TX_CTX_IDX0);
 		mtk_w32(eth, MT7628_PST_DTX_IDX0, soc->reg_map->pdma.rst_idx);
 	}
@@ -2388,7 +2454,7 @@ static void mtk_tx_clean(struct mtk_eth
 	int i;
 
 	if (ring->buf) {
-		for (i = 0; i < ring->dma_size; i++)
+		for (i = 0; i < MTK_DMA_SIZE; i++)
 			mtk_tx_unmap(eth, &ring->buf[i], false);
 		kfree(ring->buf);
 		ring->buf = NULL;
@@ -2396,14 +2462,14 @@ static void mtk_tx_clean(struct mtk_eth
 
 	if (ring->dma) {
 		dma_free_coherent(eth->dma_dev,
-				  ring->dma_size * soc->txrx.txd_size,
+				  MTK_DMA_SIZE * soc->txrx.txd_size,
 				  ring->dma, ring->phys);
 		ring->dma = NULL;
 	}
 
 	if (ring->dma_pdma) {
 		dma_free_coherent(eth->dma_dev,
-				  ring->dma_size * soc->txrx.txd_size,
+				  MTK_DMA_SIZE * soc->txrx.txd_size,
 				  ring->dma_pdma, ring->phys_pdma);
 		ring->dma_pdma = NULL;
 	}
@@ -2920,7 +2986,7 @@ static void mtk_dma_free(struct mtk_eth
 			netdev_reset_queue(eth->netdev[i]);
 	if (eth->scratch_ring) {
 		dma_free_coherent(eth->dma_dev,
-				  MTK_QDMA_RING_SIZE * soc->txrx.txd_size,
+				  MTK_DMA_SIZE * soc->txrx.txd_size,
 				  eth->scratch_ring, eth->phy_scratch_ring);
 		eth->scratch_ring = NULL;
 		eth->phy_scratch_ring = 0;
@@ -3044,7 +3110,7 @@ static int mtk_start_dma(struct mtk_eth
 		if (MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2))
 			val |= MTK_MUTLI_CNT | MTK_RESV_BUF |
 			       MTK_WCOMP_EN | MTK_DMAD_WR_WDONE |
-			       MTK_CHK_DDONE_EN | MTK_LEAKY_BUCKET_EN;
+			       MTK_CHK_DDONE_EN;
 		else
 			val |= MTK_RX_BT_32DWORDS;
 		mtk_w32(eth, val, reg_map->qdma.glo_cfg);
@@ -3090,6 +3156,7 @@ static void mtk_gdm_config(struct mtk_et
 	mtk_w32(eth, 0, MTK_RST_GL);
 }
 
+#if 0
 static int mtk_device_event(struct notifier_block *n, unsigned long event, void *ptr)
 {
 	struct mtk_mac *mac = container_of(n, struct mtk_mac, device_notifier);
@@ -3138,12 +3205,17 @@ static bool mtk_uses_dsa(struct net_devi
 	return false;
 #endif
 }
+#endif
 
 static int mtk_open(struct net_device *dev)
 {
 	struct mtk_mac *mac = netdev_priv(dev);
 	struct mtk_eth *eth = mac->hw;
+#if 1
+	int err;
+#else
 	int i, err;
+#endif
 
 	err = phylink_of_phy_connect(mac->phylink, mac->of_node, 0);
 	if (err) {
@@ -3183,8 +3255,9 @@ static int mtk_open(struct net_device *d
 		refcount_inc(&eth->dma_refcnt);
 
 	phylink_start(mac->phylink);
-	netif_tx_start_all_queues(dev);
+	netif_start_queue(dev);
 
+#if 0
 	if (MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2))
 		return 0;
 
@@ -3213,7 +3286,7 @@ static int mtk_open(struct net_device *d
 
 		mtk_w32(eth, 0, MTK_CDMP_EG_CTRL);
 	}
-
+#endif
 	return 0;
 }
 
@@ -3695,12 +3768,17 @@ static int mtk_hw_init(struct mtk_eth *e
 	 */
 	val = mtk_r32(eth, MTK_CDMQ_IG_CTRL);
 	mtk_w32(eth, val | MTK_CDMQ_STAG_EN, MTK_CDMQ_IG_CTRL);
+#if 1
+	/* Enable RX VLan Offloading */
+	mtk_w32(eth, 1, MTK_CDMP_EG_CTRL);
+#else
 	if (!MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2)) {
 		val = mtk_r32(eth, MTK_CDMP_IG_CTRL);
 		mtk_w32(eth, val | MTK_CDMP_STAG_EN, MTK_CDMP_IG_CTRL);
 
 		mtk_w32(eth, 1, MTK_CDMP_EG_CTRL);
 	}
+#endif
 
 	/* set interrupt delays based on current Net DIM sample */
 	mtk_dim_rx(&eth->rx_dim.work);
@@ -3934,11 +4012,13 @@ static int mtk_free_dev(struct mtk_eth *
 		free_netdev(eth->netdev[i]);
 	}
 
+#if 0
 	for (i = 0; i < ARRAY_SIZE(eth->dsa_meta); i++) {
 		if (!eth->dsa_meta[i])
 			break;
 		metadata_dst_free(eth->dsa_meta[i]);
 	}
+#endif
 
 	return 0;
 }
@@ -3948,12 +4028,8 @@ static int mtk_unreg_dev(struct mtk_eth
 	int i;
 
 	for (i = 0; i < MTK_MAC_COUNT; i++) {
-		struct mtk_mac *mac;
 		if (!eth->netdev[i])
 			continue;
-		mac = netdev_priv(eth->netdev[i]);
-		if (MTK_HAS_CAPS(eth->soc->caps, MTK_QDMA))
-			unregister_netdevice_notifier(&mac->device_notifier);
 		unregister_netdev(eth->netdev[i]);
 	}
 
@@ -4170,6 +4246,7 @@ static int mtk_set_rxnfc(struct net_devi
 	return ret;
 }
 
+#if 0
 static u16 mtk_select_queue(struct net_device *dev, struct sk_buff *skb,
 			    struct net_device *sb_dev)
 {
@@ -4186,6 +4263,47 @@ static u16 mtk_select_queue(struct net_d
 
 	return queue;
 }
+#endif
+
+static int
+mtk_flow_offload(flow_offload_type_t type, flow_offload_t *flow,
+		flow_offload_hw_path_t *src,
+		flow_offload_hw_path_t *dest)
+{
+	struct mtk_mac *mac = NULL;
+	struct mtk_eth *eth;
+
+	/* for now offload only do support natflow */
+	if (flow->flags != 0) {
+		return -EINVAL;
+	}
+
+	if (src->dev->netdev_ops->ndo_flow_offload == mtk_flow_offload) {
+		mac = netdev_priv(src->dev);
+	} else if (dest->dev->netdev_ops->ndo_flow_offload == mtk_flow_offload) {
+		mac = netdev_priv(dest->dev);
+	} else {
+		return -EINVAL;
+	}
+
+	eth = mac->hw;
+
+	if (!eth->soc->offload_version)
+		return -EINVAL;
+
+	return mtk_flow_offload_add(eth, type, flow, src, dest);
+}
+
+static int mtk_flow_offload_check(flow_offload_hw_path_t *path)
+{
+	if (!(path->flags & FLOW_OFFLOAD_PATH_ETHERNET))
+		return -EINVAL;
+
+	if ((path->flags & FLOW_OFFLOAD_PATH_STOP)) {
+		mtk_flow_offload_stop();
+	}
+	return 0;
+}
 
 static const struct ethtool_ops mtk_ethtool_ops = {
 	.get_link_ksettings	= mtk_get_link_ksettings,
@@ -4219,10 +4337,10 @@ static const struct net_device_ops mtk_n
 #ifdef CONFIG_NET_POLL_CONTROLLER
 	.ndo_poll_controller	= mtk_poll_controller,
 #endif
-	.ndo_setup_tc		= mtk_eth_setup_tc,
+	.ndo_flow_offload       = mtk_flow_offload,
+	.ndo_flow_offload_check = mtk_flow_offload_check,
 	.ndo_bpf		= mtk_xdp,
 	.ndo_xdp_xmit		= mtk_xdp_xmit,
-	.ndo_select_queue	= mtk_select_queue,
 };
 
 static int mtk_add_mac(struct mtk_eth *eth, struct device_node *np)
@@ -4233,7 +4351,6 @@ static int mtk_add_mac(struct mtk_eth *e
 	struct phylink *phylink;
 	struct mtk_mac *mac;
 	int id, err;
-	int txqs = 1;
 
 	if (!_id) {
 		dev_err(eth->dev, "missing mac id\n");
@@ -4251,10 +4368,7 @@ static int mtk_add_mac(struct mtk_eth *e
 		return -EINVAL;
 	}
 
-	if (MTK_HAS_CAPS(eth->soc->caps, MTK_QDMA))
-		txqs = MTK_QDMA_NUM_QUEUES;
-
-	eth->netdev[id] = alloc_etherdev_mqs(sizeof(*mac), txqs, 1);
+	eth->netdev[id] = alloc_etherdev(sizeof(*mac));
 	if (!eth->netdev[id]) {
 		dev_err(eth->dev, "alloc_etherdev failed\n");
 		return -ENOMEM;
@@ -4339,7 +4453,7 @@ static int mtk_add_mac(struct mtk_eth *e
 		eth->netdev[id]->hw_features |= NETIF_F_LRO;
 
 	eth->netdev[id]->vlan_features = eth->soc->hw_features &
-		~NETIF_F_HW_VLAN_CTAG_TX;
+		~(NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX);
 	eth->netdev[id]->features |= eth->soc->hw_features;
 	eth->netdev[id]->ethtool_ops = &mtk_ethtool_ops;
 
@@ -4350,11 +4464,12 @@ static int mtk_add_mac(struct mtk_eth *e
 		eth->netdev[id]->max_mtu = MTK_MAX_RX_LENGTH - MTK_RX_ETH_HLEN;
 	else
 		eth->netdev[id]->max_mtu = MTK_MAX_RX_LENGTH_2K - MTK_RX_ETH_HLEN;
-
+#if 0
 	if (MTK_HAS_CAPS(eth->soc->caps, MTK_QDMA)) {
 		mac->device_notifier.notifier_call = mtk_device_event;
 		register_netdevice_notifier(&mac->device_notifier);
 	}
+#endif
 
 	if (name)
 		strlcpy(eth->netdev[id]->name, name, IFNAMSIZ);
@@ -4639,6 +4754,7 @@ static int mtk_probe(struct platform_dev
 	return 0;
 
 err_deinit_mdio:
+	mtk_eth_offload_exit(eth);
 	mtk_mdio_cleanup(eth);
 err_free_dev:
 	mtk_free_dev(eth);
@@ -4656,6 +4772,8 @@ static int mtk_remove(struct platform_de
 	struct mtk_mac *mac;
 	int i;
 
+	mtk_eth_offload_exit(eth);
+
 	/* stop all devices to make sure that dma is properly shut down */
 	for (i = 0; i < MTK_MAC_COUNT; i++) {
 		if (!eth->netdev[i])
--- a/drivers/net/ethernet/mediatek/mtk_eth_soc.h
+++ b/drivers/net/ethernet/mediatek/mtk_eth_soc.h
@@ -48,6 +48,7 @@
 #define MTK_HW_FEATURES		(NETIF_F_IP_CSUM | \
 				 NETIF_F_RXCSUM | \
 				 NETIF_F_HW_VLAN_CTAG_TX | \
+				 NETIF_F_HW_VLAN_CTAG_RX | \
 				 NETIF_F_SG | NETIF_F_ALL_TSO | \
 				 NETIF_F_IPV6_CSUM |\
 				 NETIF_F_HW_TC)
@@ -290,6 +291,8 @@
 #define MTK_STAT_OFFSET		0x40
 
 /* QDMA TX NUM */
+#define MTK_QDMA_TX_NUM		16
+#define MTK_QDMA_TX_MASK	(MTK_QDMA_TX_NUM - 1)
 #define QID_BITS_V2(x)		(((x) & 0x3f) << 16)
 #define MTK_QDMA_GMAC2_QID	8
 
@@ -334,7 +337,7 @@
 #define RX_DMA_VTAG		BIT(15)
 
 /* QDMA descriptor rxd3 */
-#define RX_DMA_VID(x)		((x) & VLAN_VID_MASK)
+#define RX_DMA_VID(x)		((x) & 0x1fff) /* ext hnat need this for hash HWNAT_QUEUE_MAPPING_HASH_MASK */
 #define RX_DMA_TCI(x)		((x) & (VLAN_PRIO_MASK | VLAN_VID_MASK))
 #define RX_DMA_VPID(x)		(((x) >> 16) & 0xffff)
 
@@ -580,6 +583,11 @@
 
 #define MTK_MAC_FSM(x)		(0x1010C + ((x) * 0x100))
 
+/* natflow.h */
+#define HWNAT_QUEUE_MAPPING_MAGIC      0x8000
+#define HWNAT_QUEUE_MAPPING_MAGIC_MASK 0xe000
+#define HWNAT_QUEUE_MAPPING_HASH_MASK  0x1fff
+
 struct mtk_rx_dma {
 	unsigned int rxd1;
 	unsigned int rxd2;
@@ -968,6 +976,7 @@ struct mtk_tx_dma_desc_info {
 	u8		vlan:1;
 	u8		first:1;
 	u8		last:1;
+	u8		natflow:1;
 };
 
 struct mtk_reg_map {
@@ -1181,7 +1190,7 @@ struct mtk_eth {
 	struct metadata_dst		*dsa_meta[MTK_MAX_DSA_PORTS];
 
 	struct mtk_ppe			*ppe[2];
-	struct rhashtable		flow_table;
+	flow_offload_t __rcu		**foe_flow_table;
 
 	struct bpf_prog			__rcu *prog;
 
@@ -1300,6 +1309,14 @@ static inline u32 mtk_get_ib2_multicast_
 	return MTK_FOE_IB2_MULTICAST;
 }
 
+static inline u32 mtk_get_ib2_mib_counter_mask(struct mtk_eth *eth)
+{
+	if (MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2))
+		return MTK_FOE_IB2_MIB_COUNTER_V2;
+
+	return MTK_FOE_IB2_MIB_COUNTER;
+}
+
 /* read the hardware status register */
 void mtk_stats_update_mac(struct mtk_mac *mac);
 
@@ -1314,9 +1331,15 @@ int mtk_gmac_sgmii_path_setup(struct mtk
 int mtk_gmac_gephy_path_setup(struct mtk_eth *eth, int mac_id);
 int mtk_gmac_rgmii_path_setup(struct mtk_eth *eth, int mac_id);
 
+void mtk_eth_offload_exit(struct mtk_eth *eth);
 int mtk_eth_offload_init(struct mtk_eth *eth);
-int mtk_eth_setup_tc(struct net_device *dev, enum tc_setup_type type,
-		     void *type_data);
+int mtk_flow_offload_add(struct mtk_eth *eth, flow_offload_type_t type,
+			 flow_offload_t *flow,
+			 flow_offload_hw_path_t *src,
+			 flow_offload_hw_path_t *dest);
+void mtk_flow_offload_stop(void);
+int mtk_offload_check_rx(struct mtk_eth *eth, struct sk_buff *skb, u32 rxd4);
+int mtk_offload_check_rx_v2(struct mtk_eth *eth, struct sk_buff *skb, u32 rxd5);
 void mtk_eth_set_dma_device(struct mtk_eth *eth, struct device *dma_dev);
 
 
--- a/drivers/net/ethernet/mediatek/mtk_ppe.h
+++ b/drivers/net/ethernet/mediatek/mtk_ppe.h
@@ -34,6 +34,7 @@
 
 /* CONFIG_MEDIATEK_NETSYS_V2 */
 #define MTK_FOE_IB1_BIND_TIMESTAMP_V2	GENMASK(7, 0)
+#define MTK_FOE_IB1_BIND_KEEPALIVE_V2	BIT(13)
 #define MTK_FOE_IB1_BIND_VLAN_LAYER_V2	GENMASK(16, 14)
 #define MTK_FOE_IB1_BIND_PPPOE_V2	BIT(17)
 #define MTK_FOE_IB1_BIND_VLAN_TAG_V2	BIT(18)
@@ -55,6 +56,7 @@ enum {
 #define MTK_FOE_IB2_PSE_QOS		BIT(4)
 #define MTK_FOE_IB2_DEST_PORT		GENMASK(7, 5)
 #define MTK_FOE_IB2_MULTICAST		BIT(8)
+#define MTK_FOE_IB2_MIB_COUNTER		BIT(10)
 
 #define MTK_FOE_IB2_WDMA_QID2		GENMASK(13, 12)
 #define MTK_FOE_IB2_MIB_CNT		BIT(15)
@@ -74,6 +76,7 @@ enum {
 #define MTK_FOE_IB2_PSE_QOS_V2		BIT(8)
 #define MTK_FOE_IB2_DEST_PORT_V2	GENMASK(12, 9)
 #define MTK_FOE_IB2_MULTICAST_V2	BIT(13)
+#define MTK_FOE_IB2_MIB_COUNTER_V2	BIT(15)
 #define MTK_FOE_IB2_WDMA_WINFO_V2	BIT(19)
 #define MTK_FOE_IB2_PORT_AG_V2		GENMASK(23, 20)
 
@@ -111,14 +114,18 @@ struct mtk_foe_mac_info {
 
 /* software-only entry type */
 struct mtk_foe_bridge {
-	u8 dest_mac[ETH_ALEN];
-	u8 src_mac[ETH_ALEN];
-	u16 vlan;
+	u32 dest_mac_hi;
 
-	struct {} key_end;
+	u16 src_mac_lo;
+	u16 dest_mac_lo;
+
+	u32 src_mac_hi;
 
 	u32 ib2;
 
+	u32 _rsv[5];
+
+	u32 udf_tsid;
 	struct mtk_foe_mac_info l2;
 };
 
@@ -318,16 +325,28 @@ struct mtk_ppe {
 	struct hlist_head *foe_flow;
 
 	struct rhashtable l2_flows;
+};
 
-	void *acct_table;
+struct mtk_ppe_account_group {
+	unsigned int hash;
+	unsigned int state;
+	unsigned long jiffies;
+	unsigned long long bytes;
+	unsigned long long packets;
+	unsigned int speed_bytes[4];
+	unsigned int speed_packets[4];
+	void *priv; /* for keepalive callback */
 };
 
+void mtk_ppe_read_mib(struct mtk_ppe *ppe, unsigned int hash, struct mtk_foe_accounting *diff);
+
 struct mtk_ppe *mtk_ppe_init(struct mtk_eth *eth, void __iomem *base,
 			     int version, int index, bool accounting);
 void mtk_ppe_start(struct mtk_ppe *ppe);
 int mtk_ppe_stop(struct mtk_ppe *ppe);
 int mtk_ppe_prepare_reset(struct mtk_ppe *ppe);
 
+#if 0
 void __mtk_ppe_check_skb(struct mtk_ppe *ppe, struct sk_buff *skb, u16 hash);
 
 static inline void
@@ -349,10 +368,14 @@ mtk_ppe_check_skb(struct mtk_ppe *ppe, s
 	ppe->foe_check_time[hash] = now;
 	__mtk_ppe_check_skb(ppe, skb, hash);
 }
+#endif
+
+struct mtk_ppe_account_group *mtk_ppe_account_group_get(u32 idx);
 
 int mtk_foe_entry_prepare(struct mtk_eth *eth, struct mtk_foe_entry *entry,
 			  int type, int l4proto, u8 pse_port, u8 *src_mac,
 			  u8 *dest_mac);
+int mtk_foe_entry_clear_ttl(struct mtk_eth *eth, struct mtk_foe_entry *entry);
 int mtk_foe_entry_set_pse_port(struct mtk_eth *eth,
 			       struct mtk_foe_entry *entry, u8 port);
 int mtk_foe_entry_set_ipv4_tuple(struct mtk_eth *eth,
@@ -371,10 +394,11 @@ int mtk_foe_entry_set_pppoe(struct mtk_e
 			    int sid);
 int mtk_foe_entry_set_wdma(struct mtk_eth *eth, struct mtk_foe_entry *entry,
 			   int wdma_idx, int txq, int bss, int wcid);
+#if 0
 int mtk_foe_entry_set_queue(struct mtk_eth *eth, struct mtk_foe_entry *entry,
 			    unsigned int queue);
-int mtk_foe_entry_commit(struct mtk_ppe *ppe, struct mtk_flow_entry *entry);
-void mtk_foe_entry_clear(struct mtk_ppe *ppe, struct mtk_flow_entry *entry);
+#endif
+int mtk_foe_entry_commit(struct mtk_ppe *ppe, struct mtk_foe_entry *entry, u16 timestamp, u32 orig_hash);
 int mtk_foe_entry_idle_time(struct mtk_ppe *ppe, struct mtk_flow_entry *entry);
 int mtk_ppe_debugfs_init(struct mtk_ppe *ppe, int index);
 struct mtk_foe_accounting *mtk_foe_entry_get_mib(struct mtk_ppe *ppe, u32 index,
--- /dev/null
+++ b/drivers/net/ethernet/mediatek/mtk_ppe1.c
@@ -0,0 +1,762 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/* Copyright (C) 2020 Felix Fietkau <nbd@nbd.name> */
+
+#include <linux/kernel.h>
+#include <linux/io.h>
+#include <linux/iopoll.h>
+#include <linux/etherdevice.h>
+#include <linux/platform_device.h>
+#include <linux/if_ether.h>
+#include <linux/if_vlan.h>
+#include <net/dsa.h>
+#include "mtk_eth_soc.h"
+#include "mtk_ppe.h"
+#include "mtk_ppe_regs.h"
+
+static struct mtk_ppe_account_group mtk_ppe_account_group_entry[64];
+
+static u32 mtk_ppe_account_group_alloc(void)
+{
+	u32 i;
+	for (i = 1; i < 64; i++) {
+		if (mtk_ppe_account_group_entry[i].state == MTK_FOE_STATE_INVALID) {
+			mtk_ppe_account_group_entry[i].state = MTK_FOE_STATE_FIN; /* mark FIN as in use begin */
+			mtk_ppe_account_group_entry[i].bytes = 0;
+			mtk_ppe_account_group_entry[i].packets = 0;
+			mtk_ppe_account_group_entry[i].jiffies = jiffies;
+			return i;
+		}
+	}
+	return 0;
+}
+
+struct mtk_ppe_account_group *mtk_ppe_account_group_get(u32 idx)
+{
+	if (idx > 0 && idx < 64) {
+		return &mtk_ppe_account_group_entry[idx];
+	}
+	return NULL;
+}
+
+static void ppe_w32(struct mtk_ppe *ppe, u32 reg, u32 val)
+{
+	writel(val, ppe->base + reg);
+}
+
+static u32 ppe_r32(struct mtk_ppe *ppe, u32 reg)
+{
+	return readl(ppe->base + reg);
+}
+
+static u32 ppe_m32(struct mtk_ppe *ppe, u32 reg, u32 mask, u32 set)
+{
+	u32 val;
+
+	val = ppe_r32(ppe, reg);
+	val &= ~mask;
+	val |= set;
+	ppe_w32(ppe, reg, val);
+
+	return val;
+}
+
+static u32 ppe_set(struct mtk_ppe *ppe, u32 reg, u32 val)
+{
+	return ppe_m32(ppe, reg, 0, val);
+}
+
+static u32 ppe_clear(struct mtk_ppe *ppe, u32 reg, u32 val)
+{
+	return ppe_m32(ppe, reg, val, 0);
+}
+
+static int mtk_ppe_wait_busy(struct mtk_ppe *ppe)
+{
+	int ret;
+	u32 val;
+
+	ret = readl_poll_timeout(ppe->base + MTK_PPE_GLO_CFG, val,
+	                         !(val & MTK_PPE_GLO_CFG_BUSY),
+	                         20, MTK_PPE_WAIT_TIMEOUT_US);
+
+	if (ret)
+		dev_err(ppe->dev, "PPE table busy");
+
+	return ret;
+}
+
+static void mtk_ppe_cache_clear(struct mtk_ppe *ppe)
+{
+	ppe_set(ppe, MTK_PPE_CACHE_CTL, MTK_PPE_CACHE_CTL_CLEAR);
+	ppe_clear(ppe, MTK_PPE_CACHE_CTL, MTK_PPE_CACHE_CTL_CLEAR);
+}
+
+static void mtk_ppe_cache_enable(struct mtk_ppe *ppe, bool enable)
+{
+	mtk_ppe_cache_clear(ppe);
+
+	ppe_m32(ppe, MTK_PPE_CACHE_CTL, MTK_PPE_CACHE_CTL_EN,
+	        enable * MTK_PPE_CACHE_CTL_EN);
+}
+
+static u32 mtk_ppe_hash_entry(struct mtk_eth *eth, struct mtk_foe_entry *e)
+{
+	u32 hv1, hv2, hv3;
+	u32 hash;
+
+	switch (FIELD_GET(MTK_FOE_IB1_PACKET_TYPE, e->ib1)) {
+	case MTK_PPE_PKT_TYPE_BRIDGE:
+		hv1 = e->bridge.src_mac_lo;
+		hv1 ^= ((e->bridge.src_mac_hi & 0xffff) << 16);
+		hv2 = e->bridge.src_mac_hi >> 16;
+		hv2 ^= e->bridge.dest_mac_lo;
+		hv3 = e->bridge.dest_mac_hi;
+		break;
+	case MTK_PPE_PKT_TYPE_IPV4_ROUTE:
+	case MTK_PPE_PKT_TYPE_IPV4_HNAPT:
+		hv1 = e->ipv4.orig.ports;
+		hv2 = e->ipv4.orig.dest_ip;
+		hv3 = e->ipv4.orig.src_ip;
+		break;
+	case MTK_PPE_PKT_TYPE_IPV6_ROUTE_3T:
+	case MTK_PPE_PKT_TYPE_IPV6_ROUTE_5T:
+		hv1 = e->ipv6.src_ip[3] ^ e->ipv6.dest_ip[3];
+		hv1 ^= e->ipv6.ports;
+
+		hv2 = e->ipv6.src_ip[2] ^ e->ipv6.dest_ip[2];
+		hv2 ^= e->ipv6.dest_ip[0];
+
+		hv3 = e->ipv6.src_ip[1] ^ e->ipv6.dest_ip[1];
+		hv3 ^= e->ipv6.src_ip[0];
+		break;
+	case MTK_PPE_PKT_TYPE_IPV4_DSLITE:
+	case MTK_PPE_PKT_TYPE_IPV6_6RD:
+	default:
+		WARN_ON_ONCE(1);
+		return MTK_PPE_HASH_MASK;
+	}
+
+	hash = (hv1 & hv2) | ((~hv1) & hv3);
+	hash = (hash >> 24) | ((hash & 0xffffff) << 8);
+	hash ^= hv1 ^ hv2 ^ hv3;
+	hash ^= hash >> 16;
+	hash <<= (ffs(eth->soc->hash_offset) - 1);
+	hash &= MTK_PPE_ENTRIES - 1;
+
+	return hash;
+}
+
+static inline struct mtk_foe_mac_info *
+mtk_foe_entry_l2(struct mtk_eth *eth, struct mtk_foe_entry *entry)
+{
+	int type = mtk_get_ib1_pkt_type(eth, entry->ib1);
+
+	if (type == MTK_PPE_PKT_TYPE_BRIDGE)
+		return &entry->bridge.l2;
+
+	if (type >= MTK_PPE_PKT_TYPE_IPV4_DSLITE)
+		return &entry->ipv6.l2;
+
+	return &entry->ipv4.l2;
+}
+
+static inline u32 *
+mtk_foe_entry_ib2(struct mtk_eth *eth, struct mtk_foe_entry *entry)
+{
+	int type = mtk_get_ib1_pkt_type(eth, entry->ib1);
+
+	if (type == MTK_PPE_PKT_TYPE_BRIDGE)
+		return &entry->bridge.ib2;
+
+	if (type >= MTK_PPE_PKT_TYPE_IPV4_DSLITE)
+		return &entry->ipv6.ib2;
+
+	return &entry->ipv4.ib2;
+}
+
+int mtk_foe_entry_prepare(struct mtk_eth *eth, struct mtk_foe_entry *entry, int type, int l4proto,
+                          u8 pse_port, u8 *src_mac, u8 *dest_mac)
+{
+	struct mtk_foe_mac_info *l2;
+	u32 ports_pad, val;
+	u32 port_ag = 0;
+
+	memset(entry, 0, sizeof(*entry));
+
+	if (MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2)) {
+		val = FIELD_PREP(MTK_FOE_IB1_STATE, MTK_FOE_STATE_BIND) |
+		      FIELD_PREP(MTK_FOE_IB1_PACKET_TYPE_V2, type) |
+		      FIELD_PREP(MTK_FOE_IB1_UDP, l4proto == IPPROTO_UDP) |
+		      MTK_FOE_IB1_BIND_CACHE_V2 | MTK_FOE_IB1_BIND_TTL_V2 |
+		      MTK_FOE_IB1_BIND_KEEPALIVE_V2;
+		entry->ib1 = val;
+
+		val = FIELD_PREP(MTK_FOE_IB2_DEST_PORT_V2, pse_port) |
+		      FIELD_PREP(MTK_FOE_IB2_PORT_AG_V2, 0xf);
+	} else {
+		int port_mg = eth->soc->offload_version > 1 ? 0 : 0x3f;
+
+		val = FIELD_PREP(MTK_FOE_IB1_STATE, MTK_FOE_STATE_BIND) |
+		      FIELD_PREP(MTK_FOE_IB1_PACKET_TYPE, type) |
+		      FIELD_PREP(MTK_FOE_IB1_UDP, l4proto == IPPROTO_UDP) |
+		      MTK_FOE_IB1_BIND_TTL |
+		      MTK_FOE_IB1_BIND_CACHE |
+		      MTK_FOE_IB1_BIND_KEEPALIVE;
+		entry->ib1 = val;
+
+		port_ag = mtk_ppe_account_group_alloc();
+
+		val = FIELD_PREP(MTK_FOE_IB2_PORT_MG, port_mg) |
+		      FIELD_PREP(MTK_FOE_IB2_PORT_AG, port_ag) |
+		      FIELD_PREP(MTK_FOE_IB2_DEST_PORT, pse_port);
+	}
+
+	if (is_multicast_ether_addr(dest_mac))
+		val |= mtk_get_ib2_multicast_mask(eth);
+
+	if (eth->soc->has_accounting)
+		val |= mtk_get_ib2_mib_counter_mask(eth);
+
+	ports_pad = 0xa5a5a500 | (l4proto & 0xff);
+	if (type == MTK_PPE_PKT_TYPE_IPV4_ROUTE)
+		entry->ipv4.orig.ports = ports_pad;
+	if (type == MTK_PPE_PKT_TYPE_IPV6_ROUTE_3T)
+		entry->ipv6.ports = ports_pad;
+
+	if (type >= MTK_PPE_PKT_TYPE_IPV4_DSLITE) {
+		entry->ipv6.ib2 = val;
+		l2 = &entry->ipv6.l2;
+	} else {
+		entry->ipv4.ib2 = val;
+		l2 = &entry->ipv4.l2;
+	}
+
+	l2->dest_mac_hi = get_unaligned_be32(dest_mac);
+	l2->dest_mac_lo = get_unaligned_be16(dest_mac + 4);
+	l2->src_mac_hi = get_unaligned_be32(src_mac);
+	l2->src_mac_lo = get_unaligned_be16(src_mac + 4);
+
+	if (type >= MTK_PPE_PKT_TYPE_IPV6_ROUTE_3T)
+		l2->etype = ETH_P_IPV6;
+	else
+		l2->etype = ETH_P_IP;
+
+	return 0;
+}
+
+int mtk_foe_entry_clear_ttl(struct mtk_eth *eth, struct mtk_foe_entry *entry)
+{
+	if (MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2)) {
+		entry->ib1 &= ~MTK_FOE_IB1_BIND_TTL_V2;
+	} else {
+		entry->ib1 &= ~MTK_FOE_IB1_BIND_TTL;
+	}
+
+	return 0;
+}
+
+int mtk_foe_entry_set_pse_port(struct mtk_eth *eth,
+                               struct mtk_foe_entry *entry, u8 port)
+{
+	u32 *ib2 = mtk_foe_entry_ib2(eth, entry);
+	u32 val = *ib2;
+
+	if (MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2)) {
+		val &= ~MTK_FOE_IB2_DEST_PORT_V2;
+		val |= FIELD_PREP(MTK_FOE_IB2_DEST_PORT_V2, port);
+	} else {
+		val &= ~MTK_FOE_IB2_DEST_PORT;
+		val |= FIELD_PREP(MTK_FOE_IB2_DEST_PORT, port);
+	}
+	*ib2 = val;
+
+	return 0;
+}
+
+int mtk_foe_entry_set_ipv4_tuple(struct mtk_eth *eth,
+                                 struct mtk_foe_entry *entry, bool egress,
+                                 __be32 src_addr, __be16 src_port,
+                                 __be32 dest_addr, __be16 dest_port)
+{
+	int type = mtk_get_ib1_pkt_type(eth, entry->ib1);
+	struct mtk_ipv4_tuple *t;
+
+	switch (type) {
+	case MTK_PPE_PKT_TYPE_IPV4_HNAPT:
+		if (egress) {
+			t = &entry->ipv4.new;
+			break;
+		}
+		fallthrough;
+	case MTK_PPE_PKT_TYPE_IPV4_DSLITE:
+	case MTK_PPE_PKT_TYPE_IPV4_ROUTE:
+		t = &entry->ipv4.orig;
+		break;
+	case MTK_PPE_PKT_TYPE_IPV6_6RD:
+		entry->ipv6_6rd.tunnel_src_ip = be32_to_cpu(src_addr);
+		entry->ipv6_6rd.tunnel_dest_ip = be32_to_cpu(dest_addr);
+		return 0;
+	default:
+		WARN_ON_ONCE(1);
+		return -EINVAL;
+	}
+
+	t->src_ip = be32_to_cpu(src_addr);
+	t->dest_ip = be32_to_cpu(dest_addr);
+
+	if (type == MTK_PPE_PKT_TYPE_IPV4_ROUTE)
+		return 0;
+
+	t->src_port = be16_to_cpu(src_port);
+	t->dest_port = be16_to_cpu(dest_port);
+
+	return 0;
+}
+
+int mtk_foe_entry_set_ipv6_tuple(struct mtk_eth *eth,
+                                 struct mtk_foe_entry *entry,
+                                 __be32 *src_addr, __be16 src_port,
+                                 __be32 *dest_addr, __be16 dest_port)
+{
+	int type = mtk_get_ib1_pkt_type(eth, entry->ib1);
+	u32 *src, *dest;
+	int i;
+
+	switch (type) {
+	case MTK_PPE_PKT_TYPE_IPV4_DSLITE:
+		src = entry->dslite.tunnel_src_ip;
+		dest = entry->dslite.tunnel_dest_ip;
+		break;
+	case MTK_PPE_PKT_TYPE_IPV6_ROUTE_5T:
+	case MTK_PPE_PKT_TYPE_IPV6_6RD:
+		entry->ipv6.src_port = be16_to_cpu(src_port);
+		entry->ipv6.dest_port = be16_to_cpu(dest_port);
+		fallthrough;
+	case MTK_PPE_PKT_TYPE_IPV6_ROUTE_3T:
+		src = entry->ipv6.src_ip;
+		dest = entry->ipv6.dest_ip;
+		break;
+	default:
+		WARN_ON_ONCE(1);
+		return -EINVAL;
+	};
+
+	for (i = 0; i < 4; i++)
+		src[i] = be32_to_cpu(src_addr[i]);
+	for (i = 0; i < 4; i++)
+		dest[i] = be32_to_cpu(dest_addr[i]);
+
+	return 0;
+}
+
+int mtk_foe_entry_set_dsa(struct mtk_eth *eth, struct mtk_foe_entry *entry, int port)
+{
+	struct mtk_foe_mac_info *l2 = mtk_foe_entry_l2(eth, entry);
+
+	l2->etype = BIT(port);
+
+	if (!(entry->ib1 & mtk_get_ib1_vlan_layer_mask(eth)))
+		entry->ib1 |= mtk_prep_ib1_vlan_layer(eth, 1);
+	else
+		l2->etype |= BIT(8);
+
+	entry->ib1 &= ~mtk_get_ib1_vlan_tag_mask(eth);
+
+	return 0;
+}
+
+int mtk_foe_entry_set_vlan(struct mtk_eth *eth, struct mtk_foe_entry *entry, int vid)
+{
+	struct mtk_foe_mac_info *l2 = mtk_foe_entry_l2(eth, entry);
+
+	switch (mtk_get_ib1_vlan_layer(eth, entry->ib1)) {
+	case 0:
+		entry->ib1 |= mtk_get_ib1_vlan_tag_mask(eth) |
+		              mtk_prep_ib1_vlan_layer(eth, 1);
+		l2->vlan1 = vid;
+		return 0;
+	case 1:
+		if (!(entry->ib1 & mtk_get_ib1_vlan_tag_mask(eth))) {
+			l2->vlan1 = vid;
+			l2->etype |= BIT(8);
+		} else {
+			l2->vlan2 = vid;
+			entry->ib1 += mtk_prep_ib1_vlan_layer(eth, 1);
+		}
+		return 0;
+	default:
+		return -ENOSPC;
+	}
+}
+
+int mtk_foe_entry_set_pppoe(struct mtk_eth *eth, struct mtk_foe_entry *entry, int sid)
+{
+	struct mtk_foe_mac_info *l2 = mtk_foe_entry_l2(eth, entry);
+
+	if (!(entry->ib1 & mtk_get_ib1_vlan_layer_mask(eth)) ||
+	        (entry->ib1 & mtk_get_ib1_vlan_tag_mask(eth)))
+		l2->etype = ETH_P_PPP_SES;
+
+	entry->ib1 |= mtk_get_ib1_ppoe_mask(eth);
+	l2->pppoe_id = sid;
+
+	return 0;
+}
+
+int mtk_foe_entry_set_wdma(struct mtk_eth *eth, struct mtk_foe_entry *entry,
+                           int wdma_idx, int txq, int bss, int wcid)
+{
+	struct mtk_foe_mac_info *l2 = mtk_foe_entry_l2(eth, entry);
+	u32 *ib2 = mtk_foe_entry_ib2(eth, entry);
+
+	if (MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2)) {
+		*ib2 &= ~MTK_FOE_IB2_PORT_MG_V2;
+		*ib2 |=  FIELD_PREP(MTK_FOE_IB2_RX_IDX, txq) |
+		         MTK_FOE_IB2_WDMA_WINFO_V2;
+		l2->winfo = FIELD_PREP(MTK_FOE_WINFO_WCID, wcid) |
+		            FIELD_PREP(MTK_FOE_WINFO_BSS, bss);
+	} else {
+		*ib2 &= ~MTK_FOE_IB2_PORT_MG;
+		*ib2 |= MTK_FOE_IB2_WDMA_WINFO;
+		if (wdma_idx)
+			*ib2 |= MTK_FOE_IB2_WDMA_DEVIDX;
+		l2->vlan2 = FIELD_PREP(MTK_FOE_VLAN2_WINFO_BSS, bss) |
+		            FIELD_PREP(MTK_FOE_VLAN2_WINFO_WCID, wcid) |
+		            FIELD_PREP(MTK_FOE_VLAN2_WINFO_RING, txq);
+	}
+
+	return 0;
+}
+
+static inline bool mtk_foe_entry_usable(struct mtk_foe_entry *entry)
+{
+	return !(entry->ib1 & MTK_FOE_IB1_STATIC) &&
+	       FIELD_GET(MTK_FOE_IB1_STATE, entry->ib1) != MTK_FOE_STATE_BIND;
+}
+
+#if 0
+int mtk_foe_entry_set_queue(struct mtk_eth *eth, struct mtk_foe_entry *entry,
+			    unsigned int queue)
+{
+	u32 *ib2 = mtk_foe_entry_ib2(eth, entry);
+
+	if (MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2)) {
+		*ib2 &= ~MTK_FOE_IB2_QID_V2;
+		*ib2 |= FIELD_PREP(MTK_FOE_IB2_QID_V2, queue);
+		*ib2 |= MTK_FOE_IB2_PSE_QOS_V2;
+	} else {
+		*ib2 &= ~MTK_FOE_IB2_QID;
+		*ib2 |= FIELD_PREP(MTK_FOE_IB2_QID, queue);
+		*ib2 |= MTK_FOE_IB2_PSE_QOS;
+	}
+
+	return 0;
+}
+#endif
+
+int mtk_foe_entry_commit(struct mtk_ppe *ppe, struct mtk_foe_entry *entry,
+                         u16 timestamp, u32 orig_hash)
+{
+	struct mtk_foe_entry *hwe;
+	u32 hash;
+
+	if (MTK_HAS_CAPS(ppe->eth->soc->caps, MTK_NETSYS_V2)) {
+		entry->ib1 &= ~MTK_FOE_IB1_BIND_TIMESTAMP_V2;
+		entry->ib1 |= FIELD_PREP(MTK_FOE_IB1_BIND_TIMESTAMP_V2,
+		                         timestamp);
+	} else {
+		entry->ib1 &= ~MTK_FOE_IB1_BIND_TIMESTAMP;
+		entry->ib1 |= FIELD_PREP(MTK_FOE_IB1_BIND_TIMESTAMP,
+		                         timestamp);
+	}
+
+	hash = mtk_ppe_hash_entry(ppe->eth, entry);
+	hwe = mtk_foe_get_entry(ppe, hash);
+	if (!mtk_foe_entry_usable(hwe)) {
+		hash++;
+		hwe = mtk_foe_get_entry(ppe, hash);
+
+		if (!mtk_foe_entry_usable(hwe))
+			return -ENOSPC;
+	}
+	if (hash != orig_hash) {
+		if (hash % 2 == 0) {
+			hwe = mtk_foe_get_entry(ppe, hash + 1);
+			if (!mtk_foe_entry_usable(hwe)) {
+				return -ENOSPC;
+			} else {
+				hash++;
+				if (hash != orig_hash) {
+					return -ENOSPC;
+				}
+			}
+		} else {
+			return -ENOSPC;
+		}
+	}
+
+	memcpy(&hwe->data, &entry->data, ppe->eth->soc->foe_entry_size - sizeof(hwe->ib1));
+	wmb();
+	hwe->ib1 = entry->ib1;
+
+	dma_wmb();
+
+	mtk_ppe_cache_clear(ppe);
+
+	return hash;
+}
+
+void mtk_ppe_read_mib(struct mtk_ppe *ppe, unsigned int hash, struct mtk_foe_accounting *diff)
+{
+	int ret;
+	u32 val, cnt_r0, cnt_r1, cnt_r2;
+
+	ppe_w32(ppe, MTK_PPE_MIB_SER_CR, hash | (1 << 16));
+	ret = readx_poll_timeout_atomic(readl, ppe->base + MTK_PPE_MIB_SER_CR, val, !(val & MTK_PPE_MIB_SER_CR_ST), 20, 10000);
+	if (ret < 0) {
+		pr_notice("mib busy, please check later\n");
+		return;
+	}
+
+	cnt_r0 = ppe_r32(ppe, MTK_PPE_MIB_SER_R0);
+	cnt_r1 = ppe_r32(ppe, MTK_PPE_MIB_SER_R1);
+	cnt_r2 = ppe_r32(ppe, MTK_PPE_MIB_SER_R2);
+
+	diff->bytes = cnt_r0 + ((u64)(cnt_r1 & 0xffff) << 32);
+	diff->packets = ((cnt_r1 & 0xffff0000) >> 16) + ((cnt_r2 & 0xffffff) << 16);
+}
+
+int mtk_ppe_prepare_reset(struct mtk_ppe *ppe)
+{
+	if (!ppe)
+		return -EINVAL;
+
+	/* disable KA */
+	ppe_clear(ppe, MTK_PPE_TB_CFG, MTK_PPE_TB_CFG_KEEPALIVE);
+	ppe_clear(ppe, MTK_PPE_BIND_LMT1, MTK_PPE_NTU_KEEPALIVE);
+	ppe_w32(ppe, MTK_PPE_KEEPALIVE, 0);
+	usleep_range(10000, 11000);
+
+	/* set KA timer to maximum */
+	ppe_set(ppe, MTK_PPE_BIND_LMT1, MTK_PPE_NTU_KEEPALIVE);
+	ppe_w32(ppe, MTK_PPE_KEEPALIVE, 0xffffffff);
+
+	/* set KA tick select */
+	ppe_set(ppe, MTK_PPE_TB_CFG, MTK_PPE_TB_TICK_SEL);
+	ppe_set(ppe, MTK_PPE_TB_CFG, MTK_PPE_TB_CFG_KEEPALIVE);
+	usleep_range(10000, 11000);
+
+	/* disable scan mode */
+	ppe_clear(ppe, MTK_PPE_TB_CFG, MTK_PPE_TB_CFG_SCAN_MODE);
+	usleep_range(10000, 11000);
+
+	return mtk_ppe_wait_busy(ppe);
+}
+
+struct mtk_ppe *mtk_ppe_init(struct mtk_eth *eth, void __iomem *base,
+                             int version, int index, bool accounting)
+{
+	const struct mtk_soc_data *soc = eth->soc;
+	struct device *dev = eth->dev;
+	struct mtk_foe_entry *foe;
+	struct mtk_ppe *ppe;
+	struct mtk_mib_entry *mib;
+	struct mtk_foe_accounting *acct;
+
+	ppe = devm_kzalloc(dev, sizeof(*ppe), GFP_KERNEL);
+	if (!ppe)
+		return NULL;
+
+	/* need to allocate a separate device, since it PPE DMA access is
+	 * not coherent.
+	 */
+	ppe->base = base;
+	ppe->eth = eth;
+	ppe->dev = dev;
+	ppe->version = version;
+	ppe->accounting = accounting;
+
+	foe = dmam_alloc_coherent(ppe->dev,
+	                          MTK_PPE_ENTRIES * soc->foe_entry_size,
+	                          &ppe->foe_phys, GFP_KERNEL);
+	if (!foe)
+		return NULL;
+
+	ppe->foe_table = foe;
+
+	if (accounting) {
+		mib = dmam_alloc_coherent(ppe->dev, MTK_PPE_ENTRIES * sizeof(*mib),
+					  &ppe->mib_phys, GFP_KERNEL);
+		if (!mib)
+			return NULL;
+
+		memset(mib, 0, MTK_PPE_ENTRIES * sizeof(*mib));
+
+		ppe->mib_table = mib;
+	}
+
+	mtk_ppe_debugfs_init(ppe, index);
+
+	return ppe;
+}
+
+static void mtk_ppe_init_foe_table(struct mtk_ppe *ppe)
+{
+	static const u8 skip[] = { 12, 25, 38, 51, 76, 89, 102 };
+	int i, k;
+
+	memset(ppe->foe_table, 0, MTK_PPE_ENTRIES * ppe->eth->soc->foe_entry_size);
+
+	if (!IS_ENABLED(CONFIG_SOC_MT7621))
+		return;
+
+	/* skip all entries that cross the 1024 byte boundary */
+	for (i = 0; i < MTK_PPE_ENTRIES; i += 128) {
+		for (k = 0; k < ARRAY_SIZE(skip); k++) {
+			struct mtk_foe_entry *hwe;
+
+			hwe = mtk_foe_get_entry(ppe, i + skip[k]);
+			hwe->ib1 |= MTK_FOE_IB1_STATIC;
+		}
+	}
+}
+
+void mtk_ppe_start(struct mtk_ppe *ppe)
+{
+	u32 val;
+
+	if (!ppe)
+		return;
+
+	memset(mtk_ppe_account_group_entry, 0, sizeof(*mtk_ppe_account_group_entry) * 64);
+
+	mtk_ppe_init_foe_table(ppe);
+	ppe_w32(ppe, MTK_PPE_TB_BASE, ppe->foe_phys);
+
+	val = MTK_PPE_TB_CFG_ENTRY_80B |
+	      MTK_PPE_TB_CFG_AGE_NON_L4 |
+	      MTK_PPE_TB_CFG_AGE_UNBIND |
+	      MTK_PPE_TB_CFG_AGE_TCP |
+	      MTK_PPE_TB_CFG_AGE_UDP |
+	      MTK_PPE_TB_CFG_AGE_TCP_FIN |
+	      FIELD_PREP(MTK_PPE_TB_CFG_SEARCH_MISS,
+	                 MTK_PPE_SEARCH_MISS_ACTION_FORWARD_BUILD) |
+	      FIELD_PREP(MTK_PPE_TB_CFG_KEEPALIVE,
+	                 MTK_PPE_KEEPALIVE_DUP_CPU) |
+	      FIELD_PREP(MTK_PPE_TB_CFG_HASH_MODE, 1) |
+	      FIELD_PREP(MTK_PPE_TB_CFG_SCAN_MODE,
+	                 MTK_PPE_SCAN_MODE_KEEPALIVE_AGE) |
+	      FIELD_PREP(MTK_PPE_TB_CFG_ENTRY_NUM,
+	                 MTK_PPE_ENTRIES_SHIFT);
+	if (MTK_HAS_CAPS(ppe->eth->soc->caps, MTK_NETSYS_V2))
+		val |= MTK_PPE_TB_CFG_INFO_SEL;
+	ppe_w32(ppe, MTK_PPE_TB_CFG, val);
+
+	ppe_w32(ppe, MTK_PPE_IP_PROTO_CHK,
+	        MTK_PPE_IP_PROTO_CHK_IPV4 | MTK_PPE_IP_PROTO_CHK_IPV6);
+
+	mtk_ppe_cache_enable(ppe, true);
+
+	val = MTK_PPE_FLOW_CFG_IP6_3T_ROUTE |
+	      MTK_PPE_FLOW_CFG_IP6_5T_ROUTE |
+	      MTK_PPE_FLOW_CFG_IP6_6RD |
+	      MTK_PPE_FLOW_CFG_IP4_NAT |
+	      MTK_PPE_FLOW_CFG_IP4_NAPT |
+	      MTK_PPE_FLOW_CFG_IP4_DSLITE |
+	      MTK_PPE_FLOW_CFG_L2_BRIDGE |
+	      MTK_PPE_FLOW_CFG_IP4_NAT_FRAG;
+	if (MTK_HAS_CAPS(ppe->eth->soc->caps, MTK_NETSYS_V2))
+		val |= MTK_PPE_MD_TOAP_BYP_CRSN0 |
+		       MTK_PPE_MD_TOAP_BYP_CRSN1 |
+		       MTK_PPE_MD_TOAP_BYP_CRSN2 |
+		       MTK_PPE_FLOW_CFG_IP4_HASH_GRE_KEY;
+	else
+		val |= MTK_PPE_FLOW_CFG_IP4_TCP_FRAG |
+		       MTK_PPE_FLOW_CFG_IP4_UDP_FRAG;
+	ppe_w32(ppe, MTK_PPE_FLOW_CFG, val);
+
+	val = FIELD_PREP(MTK_PPE_UNBIND_AGE_MIN_PACKETS, 1000) |
+	      FIELD_PREP(MTK_PPE_UNBIND_AGE_DELTA, 3);
+	ppe_w32(ppe, MTK_PPE_UNBIND_AGE, val);
+
+	val = FIELD_PREP(MTK_PPE_BIND_AGE0_DELTA_UDP, 15) |
+	      FIELD_PREP(MTK_PPE_BIND_AGE0_DELTA_NON_L4, 2);
+	ppe_w32(ppe, MTK_PPE_BIND_AGE0, val);
+
+	val = FIELD_PREP(MTK_PPE_BIND_AGE1_DELTA_TCP_FIN, 2) |
+	      FIELD_PREP(MTK_PPE_BIND_AGE1_DELTA_TCP, 15);
+	ppe_w32(ppe, MTK_PPE_BIND_AGE1, val);
+
+	val = FIELD_PREP(MTK_PPE_KEEPALIVE_TIME, 1) |
+	      FIELD_PREP(MTK_PPE_KEEPALIVE_TIME_TCP, 1) |
+	      FIELD_PREP(MTK_PPE_KEEPALIVE_TIME_UDP, 1);
+	ppe_w32(ppe, MTK_PPE_KEEPALIVE, val);
+
+	val = MTK_PPE_BIND_LIMIT0_QUARTER | MTK_PPE_BIND_LIMIT0_HALF;
+	ppe_w32(ppe, MTK_PPE_BIND_LIMIT0, val);
+
+	val = MTK_PPE_BIND_LIMIT1_FULL |
+	      FIELD_PREP(MTK_PPE_BIND_LIMIT1_NON_L4, 1);
+	ppe_w32(ppe, MTK_PPE_BIND_LIMIT1, val);
+
+	val = FIELD_PREP(MTK_PPE_BIND_RATE_BIND, 30) |
+	      FIELD_PREP(MTK_PPE_BIND_RATE_PREBIND, 1);
+	ppe_w32(ppe, MTK_PPE_BIND_RATE, val);
+
+	/* enable PPE */
+	val = MTK_PPE_GLO_CFG_EN |
+	      MTK_PPE_GLO_CFG_IP4_L4_CS_DROP |
+	      MTK_PPE_GLO_CFG_IP4_CS_DROP |
+	      MTK_PPE_GLO_CFG_FLOW_DROP_UPDATE;
+	ppe_m32(ppe, MTK_PPE_GLO_CFG, val | MTK_PPE_GLO_CFG_TTL0_DROP, val);
+
+	ppe_w32(ppe, MTK_PPE_DEFAULT_CPU_PORT, 0);
+
+	if (MTK_HAS_CAPS(ppe->eth->soc->caps, MTK_NETSYS_V2)) {
+		ppe_w32(ppe, MTK_PPE_DEFAULT_CPU_PORT1, 0xcb777);
+		ppe_w32(ppe, MTK_PPE_SBW_CTRL, 0x7f);
+	}
+
+	if (ppe->accounting && ppe->mib_phys) {
+		memset(ppe->mib_table, 0, MTK_PPE_ENTRIES * sizeof(struct mtk_mib_entry));
+		ppe_w32(ppe, MTK_PPE_MIB_TB_BASE, ppe->mib_phys);
+		ppe_m32(ppe, MTK_PPE_MIB_CFG, MTK_PPE_MIB_CFG_EN,
+			MTK_PPE_MIB_CFG_EN);
+		ppe_m32(ppe, MTK_PPE_MIB_CFG, MTK_PPE_MIB_CFG_RD_CLR,
+			MTK_PPE_MIB_CFG_RD_CLR);
+		ppe_m32(ppe, MTK_PPE_MIB_CACHE_CTL, MTK_PPE_MIB_CACHE_CTL_EN,
+			MTK_PPE_MIB_CFG_RD_CLR);
+	}
+}
+
+int mtk_ppe_stop(struct mtk_ppe *ppe)
+{
+	u32 val;
+	int i;
+
+	if (!ppe)
+		return 0;
+
+	for (i = 0; i < MTK_PPE_ENTRIES; i++) {
+		struct mtk_foe_entry *hwe = mtk_foe_get_entry(ppe, i);
+
+		hwe->ib1 = FIELD_PREP(MTK_FOE_IB1_STATE,
+		                      MTK_FOE_STATE_INVALID);
+	}
+
+	mtk_ppe_cache_enable(ppe, false);
+
+	/* disable offload engine */
+	ppe_clear(ppe, MTK_PPE_GLO_CFG, MTK_PPE_GLO_CFG_EN);
+	ppe_w32(ppe, MTK_PPE_FLOW_CFG, 0);
+
+	/* disable aging */
+	val = MTK_PPE_TB_CFG_AGE_NON_L4 |
+	      MTK_PPE_TB_CFG_AGE_UNBIND |
+	      MTK_PPE_TB_CFG_AGE_TCP |
+	      MTK_PPE_TB_CFG_AGE_UDP |
+	      MTK_PPE_TB_CFG_AGE_TCP_FIN;
+	ppe_clear(ppe, MTK_PPE_TB_CFG, val);
+
+	return mtk_ppe_wait_busy(ppe);
+}
--- a/drivers/net/ethernet/mediatek/mtk_ppe_debugfs.c
+++ b/drivers/net/ethernet/mediatek/mtk_ppe_debugfs.c
@@ -82,7 +82,6 @@ mtk_ppe_debugfs_foe_show(struct seq_file
 		struct mtk_foe_entry *entry = mtk_foe_get_entry(ppe, i);
 		struct mtk_foe_mac_info *l2;
 		struct mtk_flow_addr_info ai = {};
-		struct mtk_foe_accounting *acct;
 		unsigned char h_source[ETH_ALEN];
 		unsigned char h_dest[ETH_ALEN];
 		int type, state;
@@ -96,8 +95,6 @@ mtk_ppe_debugfs_foe_show(struct seq_file
 		if (bind && state != MTK_FOE_STATE_BIND)
 			continue;
 
-		acct = mtk_foe_entry_get_mib(ppe, i, NULL);
-
 		type = FIELD_GET(MTK_FOE_IB1_PACKET_TYPE, entry->ib1);
 		seq_printf(m, "%05x %s %7s", i,
 			   mtk_foe_entry_state_str(state),
@@ -156,11 +153,9 @@ mtk_ppe_debugfs_foe_show(struct seq_file
 		*((__be16 *)&h_dest[4]) = htons(l2->dest_mac_lo);
 
 		seq_printf(m, " eth=%pM->%pM etype=%04x"
-			      " vlan=%d,%d ib1=%08x ib2=%08x"
-			      " packets=%lld bytes=%lld\n",
+			      " vlan=%d,%d ib1=%08x ib2=%08x\n",
 			   h_source, h_dest, ntohs(l2->etype),
-			   l2->vlan1, l2->vlan2, entry->ib1, ib2,
-			   acct ? acct->packets : 0, acct ? acct->bytes : 0);
+			   l2->vlan1, l2->vlan2, entry->ib1, ib2);
 	}
 
 	return 0;
--- /dev/null
+++ b/drivers/net/ethernet/mediatek/mtk_ppe_offload1.c
@@ -0,0 +1,450 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ *  Copyright (C) 2020 Felix Fietkau <nbd@nbd.name>
+ */
+
+#include <linux/version.h>
+#include <linux/kernel.h>
+#include <linux/timer.h>
+#include <net/netfilter/nf_flow_table.h>
+#include <linux/if_ether.h>
+#include <linux/rhashtable.h>
+#include <linux/if_ether.h>
+#include <linux/ip.h>
+#include <net/flow_offload.h>
+#include <net/pkt_cls.h>
+#include <net/dsa.h>
+#include "mtk_eth_soc.h"
+#ifdef CONFIG_NET_MEDIATEK_SOC_WED
+#include "mtk_wed.h"
+#endif
+
+static void mtk_foe_entry_clear(struct mtk_ppe *ppe, u16 hash)
+{
+	struct mtk_foe_entry *hwe = mtk_foe_get_entry(ppe, hash);
+	hwe->ib1 &= ~MTK_FOE_IB1_STATE;
+	hwe->ib1 |= FIELD_PREP(MTK_FOE_IB1_STATE, MTK_FOE_STATE_INVALID);
+	dma_wmb();
+}
+
+static struct timer_list ag_timer;
+static void *ag_timer_eth =  NULL;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 15, 0)
+static void mtk_ppe_account_group_walk(unsigned long ignore)
+#else
+static void mtk_ppe_account_group_walk(struct timer_list *ignore)
+#endif
+{
+	u32 i;
+	unsigned long long bytes, packets;
+	struct mtk_ppe_account_group *ag;
+	struct mtk_eth *eth = (struct mtk_eth *)ag_timer_eth;
+	void (*func)(unsigned int, unsigned long, unsigned long, unsigned int *, unsigned int *, int, unsigned long);
+	for (i = 1; i < 64; i++) {
+		ag = mtk_ppe_account_group_get(i);
+		if (ag->state == MTK_FOE_STATE_BIND) {
+			bytes = mtk_r32(eth, 0x2000 + i * 16);
+			bytes += ((unsigned long long)mtk_r32(eth, 0x2000 + i * 16 + 4)) << 32;
+			packets = mtk_r32(eth, 0x2000 + i * 16 + 8);
+			if (bytes > 0 || packets > 0) {
+				ag->jiffies = jiffies;
+				ag->bytes += bytes;
+				ag->packets += packets;
+			}
+			ag->speed_bytes[(jiffies/HZ/2) % 4] += (unsigned int)bytes;
+			ag->speed_packets[(jiffies/HZ/2) % 4] += (unsigned int)packets;
+
+			if ((func = ag->priv) != NULL && (((jiffies/HZ) % 2 == 0 && i % 2 == 0) || ((jiffies/HZ) % 2 == 1 && i % 2 == 1)) ) {
+				struct mtk_foe_entry *entry = mtk_foe_get_entry(eth->ppe[0], ag->hash);
+				if (FIELD_GET(MTK_FOE_IB1_STATE, entry->ib1) == MTK_FOE_STATE_BIND && bytes > 0 && packets > 0) {
+					bytes = ag->bytes;
+					packets = ag->packets;
+					func(ag->hash, bytes, packets, ag->speed_bytes, ag->speed_packets, 1, jiffies);
+					ag->bytes = 0;
+					ag->packets = 0;
+				} else {
+					ag->priv = NULL;
+				}
+			}
+
+			//printk("hnat-walk-ag[%u]: hash=%u bytes=%llu packets=%llu\n", i, ag->hash, bytes, packets);
+			if (time_before(ag->jiffies + 15 * HZ, jiffies)) {
+				ag->state = MTK_FOE_STATE_INVALID;
+				//printk("hnat-walk-ag[%u]: hash=%u timeout\n", i, ag->hash);
+			}
+		} else if (ag->state == MTK_FOE_STATE_FIN) {
+			if (time_before(ag->jiffies + 15 * HZ, jiffies)) {
+				ag->state = MTK_FOE_STATE_INVALID;
+			}
+		}
+	}
+
+	mod_timer(&ag_timer, jiffies + HZ * 1);
+}
+
+static void mtk_ppe_account_group_walk_stop(void)
+{
+	u32 i;
+	struct mtk_ppe_account_group *ag;
+	for (i = 1; i < 64; i++) {
+		ag = mtk_ppe_account_group_get(i);
+		if (ag->state == MTK_FOE_STATE_BIND) {
+			ag->state = MTK_FOE_STATE_INVALID;
+		}
+	}
+}
+
+#ifdef CONFIG_NET_MEDIATEK_SOC_WED
+static int
+mtk_flow_get_wdma_info(struct net_device *dev, const u8 *addr, struct mtk_wdma_info *info)
+{
+	struct net_device_path_stack stack;
+	struct net_device_path *path;
+	int err;
+
+	if (!dev)
+		return -ENODEV;
+
+	if (!IS_ENABLED(CONFIG_NET_MEDIATEK_SOC_WED))
+		return -1;
+
+	err = dev_fill_forward_path(dev, addr, &stack);
+	if (err)
+		return err;
+
+	path = &stack.path[stack.num_paths - 1];
+	if (path->type != DEV_PATH_MTK_WDMA)
+		return -1;
+
+	info->wdma_idx = path->mtk_wdma.wdma_idx;
+	info->queue = path->mtk_wdma.queue;
+	info->bss = path->mtk_wdma.bss;
+	info->wcid = path->mtk_wdma.wcid;
+
+	return 0;
+}
+#endif
+
+static int
+mtk_offload_prepare_v4(struct mtk_eth *eth, struct mtk_foe_entry *entry,
+                       flow_offload_tuple_t *s_tuple,
+                       flow_offload_tuple_t *d_tuple,
+                       flow_offload_hw_path_t *src,
+                       flow_offload_hw_path_t *dest)
+{
+	int pse_port = 1;
+
+	if (dest->dev == eth->netdev[1])
+		pse_port = 2;
+
+	pse_port = (dest->dev->netdev_ops->ndo_flow_offload ? pse_port : 0);
+	if (dest->flags & FLOW_OFFLOAD_PATH_EXTDEV)
+		pse_port = 0;
+
+	mtk_foe_entry_prepare(eth, entry, MTK_PPE_PKT_TYPE_IPV4_HNAPT, s_tuple->l4proto,
+	                      pse_port, dest->eth_src, dest->eth_dest);
+	mtk_foe_entry_set_ipv4_tuple(eth, entry, false,
+	                             s_tuple->src_v4.s_addr, s_tuple->src_port,
+	                             s_tuple->dst_v4.s_addr, s_tuple->dst_port);
+	mtk_foe_entry_set_ipv4_tuple(eth, entry, true,
+	                             d_tuple->dst_v4.s_addr, d_tuple->dst_port,
+	                             d_tuple->src_v4.s_addr, d_tuple->src_port);
+
+	if (dest->flags & FLOW_OFFLOAD_PATH_PPPOE)
+		mtk_foe_entry_set_pppoe(eth, entry, dest->pppoe_sid);
+
+	if (dest->flags & FLOW_OFFLOAD_PATH_VLAN)
+		mtk_foe_entry_set_vlan(eth, entry, dest->vlan_id);
+
+	if (dest->flags & FLOW_OFFLOAD_PATH_BRIDGE)
+		mtk_foe_entry_clear_ttl(eth, entry);
+
+#ifdef CONFIG_NET_MEDIATEK_SOC_WED
+	if (!(dest->flags & FLOW_OFFLOAD_PATH_WED_DIS) && !dest->dev->netdev_ops->ndo_flow_offload && dest->dev->netdev_ops->ndo_fill_forward_path) {
+		int err;
+		struct mtk_wdma_info info = {};
+
+		if (mtk_flow_get_wdma_info(dest->dev, dest->eth_dest, &info) == 0) {
+			mtk_foe_entry_set_wdma(eth, entry, info.wdma_idx, info.queue, info.bss, info.wcid);
+			if (MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2)) {
+				switch (info.wdma_idx) {
+				case 0:
+					pse_port = 8;
+					break;
+				case 1:
+					pse_port = 9;
+					break;
+				default:
+					return -EINVAL;
+				}
+			} else {
+				pse_port = 3;
+			}
+
+			if (info.wdma_idx >= 0 && (err = mtk_wed_flow_add(info.wdma_idx)) < 0)
+				return err;
+		}
+	}
+#endif
+
+	if (dest->dsa_port != 0xffff) {
+		mtk_foe_entry_set_dsa(eth, entry, dest->dsa_port);
+	}
+
+	mtk_foe_entry_set_pse_port(eth, entry, pse_port);
+
+	return 0;
+}
+
+static int
+mtk_offload_prepare_v6(struct mtk_eth *eth, struct mtk_foe_entry *entry,
+                       flow_offload_tuple_t *s_tuple,
+                       flow_offload_tuple_t *d_tuple,
+                       flow_offload_hw_path_t *src,
+                       flow_offload_hw_path_t *dest)
+{
+	int pse_port = 1;
+
+	if (dest->dev == eth->netdev[1])
+		pse_port = 2;
+
+	pse_port = (dest->dev->netdev_ops->ndo_flow_offload ? pse_port : 0);
+	if (dest->flags & FLOW_OFFLOAD_PATH_EXTDEV)
+		pse_port = 0;
+
+	mtk_foe_entry_prepare(eth, entry, MTK_PPE_PKT_TYPE_IPV6_ROUTE_5T, s_tuple->l4proto,
+	                      pse_port, dest->eth_src, dest->eth_dest);
+	mtk_foe_entry_set_ipv6_tuple(eth, entry,
+	                             s_tuple->src_v6.s6_addr32, s_tuple->src_port,
+	                             s_tuple->dst_v6.s6_addr32, s_tuple->dst_port);
+
+	if (dest->flags & FLOW_OFFLOAD_PATH_PPPOE)
+		mtk_foe_entry_set_pppoe(eth, entry, dest->pppoe_sid);
+
+	if (dest->flags & FLOW_OFFLOAD_PATH_VLAN)
+		mtk_foe_entry_set_vlan(eth, entry, dest->vlan_id);
+
+	if (dest->dsa_port != 0xffff)
+		mtk_foe_entry_set_dsa(eth, entry, dest->dsa_port);
+
+	return 0;
+}
+
+int mtk_flow_offload_add(struct mtk_eth *eth, flow_offload_type_t type,
+                         flow_offload_t *flow,
+                         flow_offload_hw_path_t *src,
+                         flow_offload_hw_path_t *dest)
+{
+	flow_offload_tuple_t *otuple = &flow->tuplehash[FLOW_OFFLOAD_DIR_ORIGINAL].tuple;
+	flow_offload_tuple_t *rtuple = &flow->tuplehash[FLOW_OFFLOAD_DIR_REPLY].tuple;
+	struct mtk_foe_entry orig, reply;
+	int ohash, rhash;
+	u32 timestamp;
+	u32 ag_idx;
+	struct mtk_ppe_account_group *ag;
+
+	if (otuple->l4proto != IPPROTO_TCP && otuple->l4proto != IPPROTO_UDP)
+		return -EINVAL;
+
+	if (type == FLOW_OFFLOAD_DEL) {
+		rhash = (unsigned long)flow->timeout;
+		ohash = rhash >> 16;
+		rhash &= 0xffff;
+		mtk_foe_entry_clear(eth->ppe[0], ohash);
+		mtk_foe_entry_clear(eth->ppe[0], rhash);
+		rcu_assign_pointer(eth->foe_flow_table[ohash], NULL);
+		rcu_assign_pointer(eth->foe_flow_table[rhash], NULL);
+		synchronize_rcu();
+
+		return 0;
+	}
+
+	switch (otuple->l3proto) {
+	case AF_INET:
+		if (mtk_offload_prepare_v4(eth, &orig, otuple, rtuple, src, dest) ||
+		        mtk_offload_prepare_v4(eth, &reply, rtuple, otuple, dest, src))
+			return -EINVAL;
+		break;
+	case AF_INET6:
+		if (mtk_offload_prepare_v6(eth, &orig, otuple, rtuple, src, dest) ||
+		        mtk_offload_prepare_v6(eth, &reply, rtuple, otuple, dest, src))
+			return -EINVAL;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	timestamp = mtk_r32(eth, 0x0010) & mtk_get_ib1_ts_mask(eth);
+
+	ohash = mtk_foe_entry_commit(eth->ppe[0], &orig, timestamp, ((flow->timeout >> 16) & 0xffff));
+	if (ohash < 0)
+		return -EINVAL;
+
+	rhash = mtk_foe_entry_commit(eth->ppe[0], &reply, timestamp, ((flow->timeout >> 0) & 0xffff));
+	if (rhash < 0) {
+		mtk_foe_entry_clear(eth->ppe[0], ohash);
+		return -EINVAL;
+	}
+
+	if (!MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2)) {
+		//sync ag hash with foe hash
+		ag_idx = FIELD_GET(MTK_FOE_IB2_PORT_AG, orig.ipv4.ib2);
+		ag = mtk_ppe_account_group_get(ag_idx);
+		if (ag) {
+			ag->priv = NULL;
+			ag->hash = ohash;
+			ag->state = MTK_FOE_STATE_BIND;
+		}
+
+		ag_idx = FIELD_GET(MTK_FOE_IB2_PORT_AG, reply.ipv4.ib2);
+		ag = mtk_ppe_account_group_get(ag_idx);
+		if (ag) {
+			ag->priv = NULL;
+			ag->hash = rhash;
+			ag->state = MTK_FOE_STATE_BIND;
+		}
+	}
+
+	rcu_assign_pointer(eth->foe_flow_table[ohash], flow);
+	rcu_assign_pointer(eth->foe_flow_table[rhash], flow);
+
+	/* XXX: also the same was set in natflow
+	   rhash |= ohash << 16;
+	   flow->timeout = (void *)(unsigned long)rhash;
+	 */
+
+	return 0;
+}
+
+void mtk_flow_offload_stop(void)
+{
+	int i;
+	struct mtk_eth *eth = (struct mtk_eth *)ag_timer_eth;
+
+	if (eth) {
+		for (i = 0; i < MTK_PPE_ENTRIES; i++) {
+			rcu_assign_pointer(eth->foe_flow_table[i], NULL);
+		}
+	}
+	mtk_ppe_account_group_walk_stop();
+}
+
+static void mtk_offload_keepalive(struct mtk_eth *eth, unsigned int hash)
+{
+	flow_offload_t *flow;
+
+	rcu_read_lock();
+	flow = rcu_dereference(eth->foe_flow_table[hash]);
+	if (flow) {
+		void (*func)(unsigned int, unsigned long, unsigned long, unsigned int *, unsigned int *, int, unsigned long);
+		func = (void *)flow->priv;
+		if (MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2) && func) {
+			if ( (((jiffies/HZ) % 2 == 0 && (hash/4) % 2 == 0) || ((jiffies/HZ) % 2 == 1 && (hash/4) % 2 == 1)) ) {
+				struct mtk_foe_accounting diff = {};
+				mtk_ppe_read_mib(eth->ppe[0], hash, &diff);
+				func(hash, diff.bytes, diff.packets, NULL, NULL, 2, jiffies);
+			}
+		} else if (func) {
+			struct mtk_foe_entry *entry = mtk_foe_get_entry(eth->ppe[0], hash);
+			u32 ag_idx = FIELD_GET(MTK_FOE_IB2_PORT_AG, entry->ipv4.ib2);
+			struct mtk_ppe_account_group *ag = mtk_ppe_account_group_get(ag_idx);
+			if (ag && ag->state == MTK_FOE_STATE_BIND && ag->hash == hash && ag->priv != func) {
+				unsigned long bytes = ag->bytes;
+				unsigned long packets = ag->packets;
+				func(hash, bytes, packets, ag->speed_bytes, ag->speed_packets, 1, jiffies);
+				//printk("hnat-ag[%u]: hash=%u bytes=%llu packets=%llu\n", ag_idx, hash, bytes, packets);
+				ag->bytes -= bytes;
+				ag->packets -= packets;
+				if (ag->priv != (void *)func)
+					ag->priv = func;
+			} else {
+				func(hash, 0, 0, NULL, NULL, 1, jiffies);
+			}
+		}
+	}
+	rcu_read_unlock();
+}
+
+int mtk_offload_check_rx(struct mtk_eth *eth, struct sk_buff *skb, u32 rxd4)
+{
+	unsigned int hash;
+
+	switch (FIELD_GET(MTK_RXD4_PPE_CPU_REASON, rxd4)) {
+	case MTK_PPE_CPU_REASON_KEEPALIVE_UC_OLD_HDR:
+	case MTK_PPE_CPU_REASON_KEEPALIVE_MC_NEW_HDR:
+	case MTK_PPE_CPU_REASON_KEEPALIVE_DUP_OLD_HDR:
+		hash = FIELD_GET(MTK_RXD4_FOE_ENTRY, rxd4);
+		if (hash != MTK_RXD4_FOE_ENTRY)
+			mtk_offload_keepalive(eth, hash);
+		return -1;
+	case MTK_PPE_CPU_REASON_PACKET_SAMPLING:
+		return -1;
+	case MTK_PPE_CPU_REASON_HIT_BIND_FORCE_CPU:
+		hash = FIELD_GET(MTK_RXD4_FOE_ENTRY, rxd4);
+		skb_set_hash(skb, (HWNAT_QUEUE_MAPPING_MAGIC | hash), PKT_HASH_TYPE_L4);
+		skb->vlan_tci |= HWNAT_QUEUE_MAPPING_MAGIC;
+		skb->pkt_type = PACKET_HOST;
+		skb->protocol = htons(ETH_P_IP); /* force to ETH_P_IP */
+		fallthrough;
+	default:
+		return 0;
+	}
+}
+
+int mtk_offload_check_rx_v2(struct mtk_eth *eth, struct sk_buff *skb, u32 rxd5)
+{
+	unsigned int hash;
+
+	switch (FIELD_GET(MTK_RXD5_PPE_CPU_REASON, rxd5)) {
+	case MTK_PPE_CPU_REASON_KEEPALIVE_UC_OLD_HDR:
+	case MTK_PPE_CPU_REASON_KEEPALIVE_MC_NEW_HDR:
+	case MTK_PPE_CPU_REASON_KEEPALIVE_DUP_OLD_HDR:
+		hash = FIELD_GET(MTK_RXD5_FOE_ENTRY, rxd5);
+		if (hash != MTK_RXD5_FOE_ENTRY)
+			mtk_offload_keepalive(eth, hash);
+		return -1;
+	case MTK_PPE_CPU_REASON_PACKET_SAMPLING:
+		return -1;
+	case MTK_PPE_CPU_REASON_HIT_BIND_FORCE_CPU:
+		hash = FIELD_GET(MTK_RXD5_FOE_ENTRY, rxd5);
+		skb_set_hash(skb, (HWNAT_QUEUE_MAPPING_MAGIC | hash), PKT_HASH_TYPE_L4);
+		skb->vlan_tci |= HWNAT_QUEUE_MAPPING_MAGIC;
+		skb->pkt_type = PACKET_HOST;
+		skb->protocol = htons(ETH_P_IP); /* force to ETH_P_IP */
+		fallthrough;
+	default:
+		return 0;
+	}
+}
+
+int mtk_eth_offload_init(struct mtk_eth *eth)
+{
+	eth->foe_flow_table = devm_kcalloc(eth->dev, MTK_PPE_ENTRIES,
+	                                   sizeof(*eth->foe_flow_table),
+	                                   GFP_KERNEL);
+	if (!eth->foe_flow_table)
+		return -ENOMEM;
+	if (!MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2)) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 15, 0)
+		init_timer(&ag_timer);
+		ag_timer.data = 0;
+		ag_timer.function = mtk_ppe_account_group_walk;
+#else
+		timer_setup(&ag_timer, mtk_ppe_account_group_walk, 0);
+#endif
+		ag_timer_eth = eth;
+		mod_timer(&ag_timer, jiffies + 8 * HZ);
+	}
+
+	return 0;
+}
+
+void mtk_eth_offload_exit(struct mtk_eth *eth)
+{
+	if (!MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2))
+		del_timer(&ag_timer);
+	if (eth->foe_flow_table) {
+		devm_kfree(eth->dev, eth->foe_flow_table);
+	}
+}
--- a/drivers/net/ethernet/mediatek/mtk_ppe_regs.h
+++ b/drivers/net/ethernet/mediatek/mtk_ppe_regs.h
@@ -169,4 +169,9 @@ enum {
 
 #define MTK_PPE_SBW_CTRL			0x374
 
+#define MTK_PPE_MIB_CAH_TAG_SRH			0X354
+#define MTK_PPE_MIB_CAH_LINE_RW			0X358
+#define MTK_PPE_MIB_CAH_WDATA			0X35c
+#define MTK_PPE_MIB_CAH_RDATA			0X360
+
 #endif
--- a/drivers/net/ethernet/mediatek/mtk_wed.c
+++ b/drivers/net/ethernet/mediatek/mtk_wed.c
@@ -1313,7 +1313,7 @@ mtk_wed_ppe_check(struct mtk_wed_device
 	skb_set_mac_header(skb, 0);
 	eh = eth_hdr(skb);
 	skb->protocol = eh->h_proto;
-	mtk_ppe_check_skb(eth->ppe[dev->hw->index], skb, hash);
+	//mtk_ppe_check_skb(eth->ppe[dev->hw->index], skb, hash);
 }
 
 static void
--- a/drivers/net/ppp/ppp_generic.c
+++ b/drivers/net/ppp/ppp_generic.c
@@ -53,6 +53,10 @@
 #include <net/net_namespace.h>
 #include <net/netns/generic.h>
 
+#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)
+#include <net/netfilter/nf_flow_table.h>
+#endif
+
 #define PPP_VERSION	"2.4.2"
 
 /*
@@ -1597,6 +1601,28 @@ static int ppp_fill_forward_path(struct
 	return chan->ops->fill_forward_path(ctx, path, chan);
 }
 
+#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)
+static int ppp_flow_offload_check(flow_offload_hw_path_t *path)
+{
+	struct ppp *ppp = netdev_priv(path->dev);
+	struct ppp_channel *chan;
+	struct channel *pch;
+
+	if (ppp->flags & SC_MULTILINK)
+		return -EOPNOTSUPP;
+
+	if (list_empty(&ppp->channels))
+		return -ENODEV;
+
+	pch = list_first_entry(&ppp->channels, struct channel, clist);
+	chan = pch->chan;
+	if (!chan->ops->flow_offload_check)
+		return -EOPNOTSUPP;
+
+	return chan->ops->flow_offload_check(chan, path);
+}
+#endif /* CONFIG_NF_FLOW_TABLE */
+
 static const struct net_device_ops ppp_netdev_ops = {
 	.ndo_init	 = ppp_dev_init,
 	.ndo_uninit      = ppp_dev_uninit,
@@ -1604,6 +1630,9 @@ static const struct net_device_ops ppp_n
 	.ndo_siocdevprivate = ppp_net_siocdevprivate,
 	.ndo_get_stats64 = ppp_get_stats64,
 	.ndo_fill_forward_path = ppp_fill_forward_path,
+#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)
+	.ndo_flow_offload_check = ppp_flow_offload_check,
+#endif
 };
 
 static struct device_type ppp_type = {
--- a/drivers/net/ppp/pppoe.c
+++ b/drivers/net/ppp/pppoe.c
@@ -73,6 +73,10 @@
 #include <linux/proc_fs.h>
 #include <linux/seq_file.h>
 
+#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)
+#include <net/netfilter/nf_flow_table.h>
+#endif
+
 #include <linux/nsproxy.h>
 #include <net/net_namespace.h>
 #include <net/netns/generic.h>
@@ -995,9 +999,34 @@ static int pppoe_fill_forward_path(struc
 	return 0;
 }
 
+#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)
+static int pppoe_flow_offload_check(struct ppp_channel *chan,
+		flow_offload_hw_path_t *path)
+{
+	struct sock *sk = (struct sock *)chan->private;
+	struct pppox_sock *po = pppox_sk(sk);
+	struct net_device *dev = po->pppoe_dev;
+
+	if (sock_flag(sk, SOCK_DEAD) ||
+			!(sk->sk_state & PPPOX_CONNECTED) || !dev)
+		return -ENODEV;
+
+	path->flags |= FLOW_OFFLOAD_PATH_PPPOE;
+	path->dev = dev;
+
+	if (path->dev->netdev_ops->ndo_flow_offload_check)
+		return path->dev->netdev_ops->ndo_flow_offload_check(path);
+
+	return 0;
+}
+#endif /* CONFIG_NF_FLOW_TABLE */
+
 static const struct ppp_channel_ops pppoe_chan_ops = {
 	.start_xmit = pppoe_xmit,
 	.fill_forward_path = pppoe_fill_forward_path,
+#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)
+	.flow_offload_check = pppoe_flow_offload_check,
+#endif
 };
 
 static int pppoe_recvmsg(struct socket *sock, struct msghdr *m,
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -1027,6 +1027,20 @@ struct dev_ifalias {
 struct devlink;
 struct tlsdev_ops;
 
+#define NATFLOW_OFFLOAD_HWNAT_FAKE
+struct flow_offload_fake;
+struct flow_offload_tuple_fake;
+struct flow_offload_hw_path_fake;
+enum flow_offload_type_fake {
+	FLOW_OFFLOAD_ADD        = 0,
+	FLOW_OFFLOAD_DEL,
+};
+
+typedef struct flow_offload_fake flow_offload_t;
+typedef struct flow_offload_tuple_fake flow_offload_tuple_t;
+typedef struct flow_offload_hw_path_fake flow_offload_hw_path_t;
+typedef enum flow_offload_type_fake flow_offload_type_t;
+
 struct netdev_name_node {
 	struct hlist_node hlist;
 	struct list_head list;
@@ -1553,6 +1567,11 @@ struct net_device_ops {
 	int			(*ndo_bridge_dellink)(struct net_device *dev,
 						      struct nlmsghdr *nlh,
 						      u16 flags);
+	int			(*ndo_flow_offload_check)(flow_offload_hw_path_t *path);
+	int			(*ndo_flow_offload)(flow_offload_type_t type,
+						    flow_offload_t *flow,
+						    flow_offload_hw_path_t *src,
+						    flow_offload_hw_path_t *dest);
 	int			(*ndo_change_carrier)(struct net_device *dev,
 						      bool new_carrier);
 	int			(*ndo_get_phys_port_id)(struct net_device *dev,
--- a/include/linux/ppp_channel.h
+++ b/include/linux/ppp_channel.h
@@ -31,6 +31,9 @@ struct ppp_channel_ops {
 	int	(*fill_forward_path)(struct net_device_path_ctx *,
 				     struct net_device_path *,
 				     const struct ppp_channel *);
+#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)
+	int	(*flow_offload_check)(struct ppp_channel *, flow_offload_hw_path_t *);
+#endif
 };
 
 struct ppp_channel {
--- a/include/net/netfilter/nf_flow_table.h
+++ b/include/net/netfilter/nf_flow_table.h
@@ -176,6 +176,59 @@ struct flow_offload {
 	struct rcu_head				rcu_head;
 };
 
+#define FLOW_OFFLOAD_PATH_ETHERNET      BIT(0)
+#define FLOW_OFFLOAD_PATH_VLAN          BIT(1)
+#define FLOW_OFFLOAD_PATH_PPPOE         BIT(2)
+#define FLOW_OFFLOAD_PATH_DSA           BIT(3)
+#define FLOW_OFFLOAD_PATH_STOP          BIT(4)
+#define FLOW_OFFLOAD_PATH_EXTDEV        BIT(5)
+#define FLOW_OFFLOAD_PATH_WED_DIS       BIT(6)
+#define FLOW_OFFLOAD_PATH_BRIDGE        BIT(7)
+
+struct flow_offload_tuple_fake {
+	union {
+		struct in_addr          src_v4;
+		struct in6_addr         src_v6;
+	};
+	union {
+		struct in_addr          dst_v4;
+		struct in6_addr         dst_v6;
+	};
+	struct {
+		__be16                  src_port;
+		__be16                  dst_port;
+	};
+
+	u8                              l3proto;
+	u8                              l4proto;
+};
+
+struct flow_offload_tuple_rhash_fake {
+	struct flow_offload_tuple_fake	tuple;
+};
+
+struct flow_offload_fake {
+	struct flow_offload_tuple_rhash_fake    tuplehash[FLOW_OFFLOAD_DIR_MAX];
+	u32                                     flags;
+	u32                                     timeout;
+	union {
+		/* Your private driver data here. */
+		void *priv;
+	};
+};
+
+struct flow_offload_hw_path_fake {
+	struct net_device *dev;
+	u32 flags;
+
+	u8 eth_src[ETH_ALEN];
+	u8 eth_dest[ETH_ALEN];
+	u16 vlan_proto;
+	u16 vlan_id;
+	u16 pppoe_sid;
+	u16 dsa_port;
+};
+
 #define NF_FLOW_TIMEOUT (30 * HZ)
 #define nf_flowtable_time_stamp	(u32)jiffies
 
--- a/net/8021q/vlan_dev.c
+++ b/net/8021q/vlan_dev.c
@@ -26,6 +26,9 @@
 #include <linux/ethtool.h>
 #include <linux/phy.h>
 #include <net/arp.h>
+#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)
+#include <net/netfilter/nf_flow_table.h>
+#endif
 
 #include "vlan.h"
 #include "vlanproc.h"
@@ -799,6 +802,25 @@ static int vlan_dev_fill_forward_path(st
 	return 0;
 }
 
+#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)
+static int vlan_dev_flow_offload_check(flow_offload_hw_path_t *path)
+{
+        struct net_device *dev = path->dev;
+        struct vlan_dev_priv *vlan = vlan_dev_priv(dev);
+
+        if (path->flags & FLOW_OFFLOAD_PATH_VLAN)
+                return -EEXIST;
+
+        path->flags |= FLOW_OFFLOAD_PATH_VLAN;
+        path->dev = vlan->real_dev;
+
+        if (vlan->real_dev->netdev_ops->ndo_flow_offload_check)
+                return vlan->real_dev->netdev_ops->ndo_flow_offload_check(path);
+
+        return 0;
+}
+#endif /* CONFIG_NF_FLOW_TABLE */
+
 static const struct ethtool_ops vlan_ethtool_ops = {
 	.get_link_ksettings	= vlan_ethtool_get_link_ksettings,
 	.get_drvinfo	        = vlan_ethtool_get_drvinfo,
@@ -838,6 +860,9 @@ static const struct net_device_ops vlan_
 	.ndo_fix_features	= vlan_dev_fix_features,
 	.ndo_get_iflink		= vlan_dev_get_iflink,
 	.ndo_fill_forward_path	= vlan_dev_fill_forward_path,
+#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)
+	.ndo_flow_offload_check = vlan_dev_flow_offload_check,
+#endif
 };
 
 static void vlan_dev_free(struct net_device *dev)
--- a/net/bridge/br_device.c
+++ b/net/bridge/br_device.c
@@ -14,6 +14,9 @@
 #include <linux/ethtool.h>
 #include <linux/list.h>
 #include <linux/netfilter_bridge.h>
+#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)
+#include <net/netfilter/nf_flow_table.h>
+#endif
 
 #include <linux/uaccess.h>
 #include "br_private.h"
@@ -437,6 +440,28 @@ static int br_fill_forward_path(struct n
 	return 0;
 }
 
+#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)
+static int br_flow_offload_check(flow_offload_hw_path_t *path)
+{
+	struct net_device *dev = path->dev;
+	struct net_bridge *br = netdev_priv(dev);
+	struct net_bridge_fdb_entry *dst;
+
+	if (!(path->flags & FLOW_OFFLOAD_PATH_ETHERNET))
+		return -EINVAL;
+
+	dst = br_fdb_find_rcu(br, path->eth_dest, path->vlan_id);
+	if (!dst || !dst->dst)
+		return -ENOENT;
+
+	path->dev = dst->dst->dev;
+	if (path->dev->netdev_ops->ndo_flow_offload_check)
+		return path->dev->netdev_ops->ndo_flow_offload_check(path);
+
+	return 0;
+}
+#endif /* CONFIG_NF_FLOW_TABLE */
+
 static const struct ethtool_ops br_ethtool_ops = {
 	.get_drvinfo		 = br_getinfo,
 	.get_link		 = ethtool_op_get_link,
@@ -472,6 +497,9 @@ static const struct net_device_ops br_ne
 	.ndo_bridge_dellink	 = br_dellink,
 	.ndo_features_check	 = passthru_features_check,
 	.ndo_fill_forward_path	 = br_fill_forward_path,
+#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)
+	.ndo_flow_offload_check  = br_flow_offload_check,
+#endif
 };
 
 static struct device_type br_type = {
--- a/net/dsa/slave.c
+++ b/net/dsa/slave.c
@@ -20,6 +20,9 @@
 #include <linux/if_bridge.h>
 #include <linux/if_hsr.h>
 #include <linux/netpoll.h>
+#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)
+#include <net/netfilter/nf_flow_table.h>
+#endif
 
 #include "dsa_priv.h"
 
@@ -1743,6 +1746,27 @@ static int dsa_slave_fill_forward_path(s
 	return 0;
 }
 
+#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)
+static int dsa_flow_offload_check(flow_offload_hw_path_t *path)
+{
+	struct net_device *dev = path->dev;
+	struct dsa_port *dp;
+
+	if (!(path->flags & FLOW_OFFLOAD_PATH_ETHERNET))
+		return -EINVAL;
+
+	dp = dsa_slave_to_port(dev);
+	path->dsa_port = dp->index;
+	path->dev = dsa_slave_to_master(dev);
+	path->flags |= FLOW_OFFLOAD_PATH_DSA;
+
+	if (path->dev->netdev_ops->ndo_flow_offload_check)
+		return path->dev->netdev_ops->ndo_flow_offload_check(path);
+
+	return 0;
+}
+#endif /* CONFIG_NF_FLOW_TABLE */
+
 static const struct net_device_ops dsa_slave_netdev_ops = {
 	.ndo_open	 	= dsa_slave_open,
 	.ndo_stop		= dsa_slave_close,
@@ -1767,6 +1791,9 @@ static const struct net_device_ops dsa_s
 	.ndo_get_devlink_port	= dsa_slave_get_devlink_port,
 	.ndo_change_mtu		= dsa_slave_change_mtu,
 	.ndo_fill_forward_path	= dsa_slave_fill_forward_path,
+#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)
+	.ndo_flow_offload_check  = dsa_flow_offload_check,
+#endif
 };
 
 static struct device_type dsa_type = {
